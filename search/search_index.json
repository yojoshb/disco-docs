{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Installing OpenShift in a Disconnected/Air-Gapped Environment: Streamlined","text":"<p>This documents purpose is for consolidating information to stand up a OpenShift v4.14 and later cluster in a disconnected/air-gap environment using the agent-based installer on bare metal/virtual machine. </p> <p>This in not a one-size fits all, this is not specific to any team/org, this is rough and simple to at least put the important information in one centralized document. </p> <p>This documentation was written for OpenShift v4.17, but you can follow this process for any version later than v4.14, just sub in whatever version of the platform you want.</p> <ul> <li>Relevant docs/articles are linked in each section if applicable. </li> <li>Commands are be included within the documention as well as any associated output. </li> </ul> <p></p>"},{"location":"prereqs.html","title":"Getting started","text":"<p>The general flow of a disconnected OpenShift install goes like this: </p> <p>Connected Process</p> <ol> <li>Download the required tools from Red Hat.</li> <li>Create an image set configuration file.</li> <li>Mirror an image set to disk using oc-mirror.</li> <li>Transfer the image set and associated tools to the target disconnected environment.</li> </ol> <p>Disconnected Process</p> <ol> <li>Set up the tools and create a push/pull secret for your target mirror registry.</li> <li>Upload the image set to the disconnected mirror registry.</li> <li>Create the cluster configs and build the ISO using the Agent-based Installer.</li> <li>Boot the the ISO onto your hardware.</li> <li>Configure your cluster to use the resources generated by the oc-mirror plugin.</li> </ol>"},{"location":"prereqs.html#basic-prerequisites","title":"Basic Prerequisites","text":"<p>Online connected (low-side)</p> <ul> <li>A Red Hat account and valid OpenShift subscription</li> <li>RHEL 8/9 or compatible WSL machine that can access the internet (Red Hat's CDN, registry.redhat.io, quay.io)</li> <li>Adequate disk space either on the machine and transfer disk, or just the transfer disk: 30GB-100GB+ (Dependant on what you want to mirror/download)</li> <li>Data transfer capabilities (connected to disconnected)</li> </ul> <p>Offline disconnected (high-side) </p> <ul> <li>A machine that can access the network that the cluster will be installed to</li> <li>Adequate disk space on the disconnected machine and transfer disk, or just the transfer disk: 30GB-100GB+ (Dependant on what you bring over from the connected network)</li> <li> <p>A docker v2-2 capable registry with adequate storage space: 30GB-100GB+ (If you do not have a registry in your environment, you can use the mirror registry for Red Hat OpenShift)</p> <p>If you want to use the Red Hat provided mirror registry, the machine must be able to run Podman. Changes to the machine may need to happen that may violate the DISA STIG.</p> </li> <li> <p>DNS server</p> </li> <li>NTP server/source</li> </ul>"},{"location":"prereqs.html#cluster-resources","title":"Cluster Resources","text":"<p>Recommended cluster resources for the following topologies:</p> Topology # of master nodes # of worker nodes vCPU Memory Storage Single-node cluster 1 0 16 vCPUs 32 GB of RAM 120 GB Compact cluster 3 0 or 1 8 vCPUs 16 GB of RAM 120 GB HA cluster 3 2 and above 8 vCPUs 16 GB of RAM 120 GB"},{"location":"prereqs.html#enclave-support","title":"Enclave Support","text":"<p>Red Hat Docs</p> <p>You can also mirror into an enclave by following the docs linked above. This allows you to mirror for multiple disconnected environments within your organization.</p>"},{"location":"prereqs.html#fips-compliance","title":"FIPS Compliance","text":"<p>OpenShift version 4.12 to 4.15 </p> <p>To enable FIPS mode for your cluster, you must run the installation program from a RHEL 8 computer that is configured to operate in FIPS mode. Running RHEL 9 with FIPS mode enabled to install an OpenShift Container Platform cluster is not possible. </p> <ul> <li>Red Hat Docs</li> </ul> <p>OpenShift version 4.16 and later </p> <p>To enable FIPS mode for your cluster, you must run the installation program from a RHEL 9 computer that is configured to operate in FIPS mode, and you must use a FIPS-capable version of the installation program. </p> <ul> <li>Red Hat Docs</li> </ul>"},{"location":"release.html","title":"About","text":"<p>Version: 0.7.1</p>"},{"location":"release.html#additional-resources","title":"Additional Resources","text":"<p>Some additional resources to continue on your journey in no particular order:</p> <ul> <li>https://github.com/yojoshb/ocp/</li> <li>https://github.com/bstrauss84/openshift-install-configs/</li> <li>https://github.com/afouladi7/openshift_shortcuts/</li> <li>https://github.com/yakovbeder/oc-mirror-web-app/</li> <li>https://github.com/kenmoini/disconnected-openshift/</li> <li>https://github.com/kenmoini/ansible-redfish/</li> <li>https://github.com/RedHatGov/ocp/</li> </ul>"},{"location":"connected/index.html","title":"Connected Setup","text":"<p>Red Hat Docs</p> <p>The connected Linux machine will need to be able to run provided tools and access specific registries.</p> <ul> <li> <p>The current CDN access needed for the mirror host:</p> <p>Red Hat KCS, Red Hat Docs</p> URL Port Function <code>registry.redhat.io</code> 443 Provides core container images <code>access.redhat.com</code> 443 Hosts a signature store that a container client requires for verifying images pulled from <code>registry.access.redhat.com</code>. In a firewall environment, ensure that this resource is on the allowlist. <code>registry.access.redhat.com</code> 443 Hosts all the container images that are stored on the Red Hat Ecosystem Catalog, including core container images. <code>quay.io</code> 443 Provides core container images <code>cdn.quay.io</code> 443 Provides core container images <code>cdn01.quay.io</code> 443 Provides core container images <code>cdn02.quay.io</code> 443 Provides core container images <code>cdn03.quay.io</code> 443 Provides core container images <code>cdn04.quay.io</code> 443 Provides core container images <code>cdn05.quay.io</code> 443 Provides core container images <code>cdn06.quay.io</code> 443 Provides core container images <code>sso.redhat.com</code> 443 <code>https://cloud.redhat.com/openshift</code> site uses authentication from <code>sso.redhat.com</code>. <code>mirror.openshift.com</code> 443 Required to access mirrored installation content and images. </li> </ul> <p>The general flow of a disconnected OpenShift install starts with: </p> <ol> <li>Download the required tools from Red Hat.</li> <li>Create an image set configuration file defining what you want to mirror.</li> <li>Mirror an image set to disk using oc mirror.</li> <li>Transfer the image set and associated tools to the target environment.</li> </ol>"},{"location":"connected/imagesets.html","title":"1.2 Creating imagesets","text":""},{"location":"connected/imagesets.html#creating-imagesets-for-mirroring","title":"Creating ImageSets for mirroring","text":"<ol> <li> <p>Let's browse the catalog to determine what we want to download using <code>oc mirror</code></p> <p>Tip</p> <p>Some of these commands take time, and by default they print to stdout. You can send the outputs to a file e.g. '<code>oc mirror list operators &gt; $(date +%F)-openshift-operators.txt</code>' or view the stdout in <code>.oc-mirror.log</code>. </p> <p>This handy little one-liner will dump all available operators and their corresponding catalogs and store them in text files for a specific version. This will take some time to run initially, but saves a bunch of time in the long run</p> <p>Source: Allens Repository with more helpful scripts and configs</p> <pre><code>for i in $(oc-mirror list operators --catalogs --version=4.17 | grep registry); do $(oc-mirror list operators --catalog=$i --version=4.17 &gt; $(echo $i | cut -b 27- | rev | cut -b 7- | rev).txt); done\n</code></pre> <p>The <code>.oc-mirror.log</code> file gets generated in the current directory you are at when you run oc mirror commands.</p> <ul> <li>Using <code>oc mirror list releases</code> to list platform releases and versions</li> </ul> <pre><code># Output OpenShift release versions\noc mirror list releases\n\n# Output all OpenShift release channels list for a release\noc mirror list releases --version=4.17\n\n# List all OpenShift versions in a specified channel\noc mirror list releases --channel=stable-4.17\n\n# List all OpenShift channels for a specific version\noc mirror list releases --channels --version=4.17\n\n# List OpenShift channels for a specific version and one or more release architecture. \n# Valid architectures: amd64 (default), arm64, ppc64le, s390x, multi.\noc mirror list releases --channels --version=4.17 --filter-by-archs amd64,arm64,ppc64le,s390x,multi\n</code></pre> <ul> <li>Using <code>oc mirror list operators</code> to list available operator catalog content and versions</li> </ul> <pre><code># List available operator catalog release versions\noc mirror list operators\n\n# Output default operator catalogs for OpenShift release 4.17\noc mirror list operators --catalogs --version=4.17\n\n# List all operator packages in a catalog\noc mirror list operators --catalog=catalog-name\n\n# List all channels in an operator package\noc mirror list operators --catalog=catalog-name --package=package-name\n\n# List all available versions for a specified operator in a channel\noc mirror list operators --catalog=catalog-name --package=operator-name --channel=channel-name\n</code></pre> </li> <li> <p>Create a imageset configuration file to define what you want to mirror. You can add comments to this file if you would like as the file can get quite busy. You can name the file anything you want as long as the yaml formatting is correct e.g. <code>4.17-imageset-config.yaml</code>. </p> <p>Be aware of yaml formatting, line indentation matters</p> <ul> <li>Red Hat Doc examples</li> <li>The example below is for OpenShift 4.17 stable, with the <code>lvms-operator</code> to use node attached disks for persistent storage and the <code>cincinnati-operator</code> to make cluster updating easier using OSUS consuming graph data pulled from Red Hat.</li> <li>You can look at several imageset-config.yaml examples here in this document</li> </ul> Example: imageset-config.yaml<pre><code>kind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  platform:\n    architectures:\n    - amd64 # (1)!\n    channels:\n    - type: ocp\n      name: stable-4.17 # (2)!\n    graph: true # (3)! \n\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17 # (4)!\n    packages:\n    - name: lvms-operator # (5)!\n      defaultChannel: stable-4.17 # (6)!\n      channels:\n      - name: stable-4.17 # (7)!\n    - name: cincinnati-operator # (8)!\n\n  additionalImages: # (9)!\n  - name: registry.redhat.io/ubi8/ubi:latest\n  - name: registry.redhat.io/openshift4/ose-must-gather:latest\n\n  helm: {} # (10)!\n</code></pre> <ol> <li>Defines what architecture platform images we want to download: This will help decrease the size of the mirrored content</li> <li>Defines what version we want to download: OCP version 4.17 stable branch</li> <li>Graph data needed for the cincinnati operator</li> <li>Operator catalog we want to download from</li> <li>Operator that we want to download</li> <li>Some operators will need a defaultChannel specified. oc-mirror will tell you if it's required when attempting to mirror the data</li> <li>Version/Channel for said operators</li> <li>Only has one channel so just take the latest version</li> <li>Any additional images to bring with us, RHEL UBI's are always nice to have for testing, ose-must-gather is for gathering extra logs in the event a must gather is used for support or general debugging</li> <li>Any additional helm charts, these are non-cataloged application bundles more or less. This example we aren't specifying anything so it's blank</li> </ol> </li> </ol>"},{"location":"connected/mirroring.html","title":"1.3 Mirror images to disk","text":""},{"location":"connected/mirroring.html#mirroring-images-to-disk","title":"Mirroring images to disk","text":"<p>Red Hat Docs</p> <p>Now that the images are defined, we can mirror them to disk. Repeat this process for updates or additions to your cluster.</p> <ul> <li>Pass in the image set configuration file that was created. This procedure assumes that it is named <code>imageset-config.yaml</code>. If you named your's differently, sub in your name of the file.</li> <li>Specify the target directory where you want to output the image set tar file. The target directory path must start with <code>file://</code>. This procedure assumes you want to store the image set in <code>/opt/4.17-mirrordata</code>. Store it anywhere that has available disk space. Can even be the mounted drive you're going to use to transfer the data to the high-side.</li> <li>The target directory will also hold the <code>working-dir</code> environment. This directory contains the various necessary data to build, update, and maintain cluster resources. Keep this directory safe, and do not modify it. It will be used again for updates and additions to your cluster</li> <li>Be aware of the caching system, this will also take up considerable space on the disk depending on how many images you want to mirror</li> </ul> <p>Caching</p> <ul> <li>How does the cache work?<ul> <li>It's like a local registry, it can take up additional disk space almost as large as the .tar that gets generated</li> </ul> </li> <li>Where is it saved?<ul> <li>By default in <code>$HOME/.oc-mirror/.cache</code></li> </ul> </li> <li>Can I control where I want the cache to be stored?<ul> <li>Yes, you can pass <code>--cache-dir &lt;dir&gt;</code> which will change the cache location to <code>&lt;dir&gt;/.oc-mirror/.cache</code></li> </ul> </li> <li>During the mirroring process, is there a way to resume if something goes wrong?<ul> <li>Yes, by re-running oc mirror</li> </ul> </li> <li>I intentionally canceled the task and re-ran the mirroring process, but it seemed to start from the beginning.<ul> <li>It goes through the images from your ISC but it won't pull them if they're already in the cache. You can compare the elapsed times by running a second time with the images already cached.</li> </ul> </li> <li>The cache takes up a lot of disk space can it be deleted?<ul> <li>Yes the cache can be removed, oc mirror will just re-download what's needed</li> </ul> </li> </ul> <ol> <li> <p>You have set the umask parameter to <code>0022</code> on the operating system that uses oc-mirror.</p> <pre><code>umask 0022\n</code></pre> </li> <li> <p>Perform a dry-run of the mirror to disk process to verify your imageset-config is valid and the tools can gather the data</p> <p>Info</p> <p>Ignore the warning for <code>images necessary for mirroring are not available in the cache.</code> This is just letting you know nothing has actually been downloaded to the cache yet as this is a dry-run, except the graph-data image...</p> <p>Currently the <code>--dry-run</code> process still downloads the graph-image to cache</p> <p><pre><code>oc mirror -c imageset-config.yaml file:///opt/4.17-mirrordata --dry-run --v2\n</code></pre> Example Output<pre><code>[INFO]   : \ud83d\udc4b Hello, welcome to oc-mirror\n[INFO]   : \u2699\ufe0f  setting up the environment for you...\n[INFO]   : \ud83d\udd00 workflow mode: mirrorToDisk\n[INFO]   : \ud83d\udd75  going to discover the necessary images...\n[INFO]   : \ud83d\udd0d collecting release images...\n[INFO]   : \ud83d\udd0d collecting operator images...\n\u2713   (2m53s) Collecting catalog registry.redhat.io/redhat/redhat-operator-index:v4.17\n[INFO]   : \ud83d\udd0d collecting additional images...\n[INFO]   : \ud83d\udd0d collecting helm images...\n[WARN]   : \u26a0\ufe0f  193/194 images necessary for mirroring are not available in the cache.\n[WARN]   : List of missing images in : /opt/4.17-mirrordata/working-dir/dry-run/missing.txt.\nplease re-run the mirror to disk process\n[INFO]   : \ud83d\udcc4 list of all images for mirroring in : /opt/4.17-mirrordata/working-dir/dry-run/mapping.txt\n[INFO]   : mirror time     : 24.392878858s\n[INFO]   : \ud83d\udc4b Goodbye, thank you for using oc-mirror\n</code></pre></p> </li> <li> <p>Perform the mirror to disk process   <pre><code>oc mirror -c imageset-config.yaml file:///opt/4.17-mirrordata --v2\n</code></pre> Example Output<pre><code>...\n[INFO]   : === Results ===\n[INFO]   :  \u2713  185 / 185 release images mirrored successfully\n[INFO]   :  \u2713  8 / 8 operator images mirrored successfully\n[INFO]   :  \u2713  1 / 1 additional images mirrored successfully\n[INFO]   : \ud83d\udce6 Preparing the tarball archive...\n[INFO]   : mirror time     : 13m31.071692892s\n[INFO]   : \ud83d\udc4b Goodbye, thank you for using oc-mirror\n</code></pre></p> </li> <li> <p>List your output directory and verify the image set <code>mirror_000001.tar</code> file was created. The <code>working-dir</code> will contain logs and relavent info for the data mirrorred to disk.     <pre><code>ls /opt/4.17-mirrordata/\n</code></pre> Example Output<pre><code>mirror_000001.tar  working-dir\n</code></pre></p> </li> </ol>"},{"location":"connected/mirroring.html#extract-the-openshift-install-binary-from-the-release-images-mirrored","title":"Extract the openshift-install binary from the release-images mirrored","text":"<p>This binary will be used to create the ISO that you will boot on your hardware to install the cluster. This binary has to match the exact version of OpenShift release images that you have mirrored. To extract the openshift-install that's built for your mirrored images version on the connected network:</p> <p>Note</p> <p>If you used the <code>rhel-oc-tools.sh</code> script and chose to extract the binary you do not need to do this.</p> <ol> <li> <p>Construct the correct URL to download the payload for your release images. Follow one of these steps</p> <ol> <li>Using oc-mirror v2 directory structure, get the hash from the images we mirrored to disk <pre><code>ls /opt/4.17-mirrordata/working-dir/signatures/4.17.70-x86_64-sha256-d9c985464c0315160971b3e79f5fbec628d403a572f7a6d893c04627c066c0bb | awk -F'sha256-' '{print $2}'\n40a0dce2a37e3278adc2fd64f63bca67df217b7dd8f68575241b66fdae1f04a3\n\n# Store this in a variable to use it to curate the URL to extract the installer from without copying and pasting \nexport HASH=$(ls /opt/4.17-mirrordata/working-dir/signatures/4.17.70-x86_64-sha256-40a0dce2a37e3278adc2fd64f63bca67df217b7dd8f68575241b66fdae1f04a3 | awk -F'sha256-' '{print $2}')\n</code></pre></li> <li>Or, construct the entire URL based on the version specified in the imageset config file. Read the warning below to understand the potential issue you could run into with this method. <pre><code>export VERSION=stable-4.17\nexport RELEASE_ARCH=amd64\nexport RELEASE_IMAGE=$(curl -s https://mirror.openshift.com/pub/openshift-v4/$RELEASE_ARCH/clients/ocp/$VERSION/release.txt | grep 'Pull From: quay.io' | awk -F ' ' '{print $3}')\necho $RELEASE_IMAGE\nquay.io/openshift-release-dev/ocp-release@sha256:40a0dce2a37e3278adc2fd64f63bca67df217b7dd8f68575241b66fdae1f04a3\n</code></pre></li> </ol> <p>Warning</p> <p>Be aware that this could end up downloading a different version of installer if you mirrored the images at a earlier time.</p> <p>Example: You mirrored the <code>stable-4.17</code> the images a week ago, at that time the stable channel was set to version <code>4.17.70</code>. Now you go to extract the binary a week later, but the stable-4.17 branch was updated from <code>4.17.70</code> to <code>4.17.71</code>. The binary would be downloaded for the newer stable branch <code>4.17.71</code> and be the incorrect version with the images you mirrored prior. This would cause the installer to fail, as it would be looking for the newer release payload on the mirror registry.</p> </li> <li> <p>Use <code>oc adm</code> to extract the openshift-install binary that is purpose built for the version of images you mirrored. This command will extract the <code>openshift-install</code> or <code>openshift-install-fips</code> binary to your current directory. You can pass in the <code>--dir='&lt;path&gt;'</code> to extract the binary to a specific location on your filesystem.      <pre><code>oc adm release extract --command=openshift-install quay.io/openshift-release-dev/ocp-release@sha256:$HASH\n\n# If you are installing OpenShift 4.16 or later and requiring FIPS\noc adm release extract --command=openshift-install-fips quay.io/openshift-release-dev/ocp-release@sha256:$HASH\n</code></pre></p> <p>If you constructed the entire URL <pre><code>oc adm release extract --command=openshift-install $RELEASE_IMAGE\n\n# If you are installing OpenShift 4.16 or later and requiring FIPS\noc adm release extract --command=openshift-install-fips $RELEASE_IMAGE\n</code></pre></p> </li> <li> <p>Now check the SHA256 hash against the release signatures we looked at before by running <code>openshift-install version</code> using the binary you just downloaded.     <pre><code>./openshift-install version\n</code></pre> Example Output<pre><code>./openshift-install 4.12.70\nbuilt from commit 798aeaaf61fbc22669b6bad2edc058ea6949d733\nrelease image quay.io/openshift-release-dev/ocp-release@sha256:40a0dce2a37e3278adc2fd64f63bca67df217b7dd8f68575241b66fdae1f04a3\nrelease architecture amd64\n</code></pre></p> </li> </ol> <p>If the SHA256 values match each other (highlighted values), then you have extracted the correct <code>openshift-install</code> binary that can build your cluster ISO with the release images you mirrored. If the hashes do not match, you will need to either go find the correct <code>openshift-install</code> binary version for your images, or mirror the images again to the correct version of the install binary.</p>"},{"location":"connected/mirroring.html#transfer-data-and-tools-to-the-disconnected-environment","title":"Transfer data and tools to the disconnected environment.","text":"<p>Place these files on a disk and transfer them to your disconnected network</p> <ul> <li>mirror_000001.tar (image set .tar file)</li> <li>imageset-config.yaml (your image set config file)</li> <li>oc</li> <li>oc-mirror</li> <li>butane</li> <li>mirror-registry (if using)</li> <li>openshift-install or openshift-install-fips</li> </ul>"},{"location":"connected/tools.html","title":"1.1 Tools","text":""},{"location":"connected/tools.html#download-necessary-tools","title":"Download necessary tools","text":"<p>Be aware of the tool(s) version and architecture. Certain tools require matching to the version of OpenShift you're installing and the correct binary for the RHEL version that you are using. </p> <ul> <li>Red Hat Console Downloads has the latest-stable version of all the tools. </li> <li>Look through the public mirror site if you need to get a specific version. Red Hat console downloads come from this site.</li> <li> <p>Or access specific versions through the OCP downloads page on the Red Hat Customer Portal</p> <p>Info</p> <p>You can use the <code>rhel-oc-tools.sh</code> script in the docs repository that will download all the tools for you if you'd like. Make sure to edit the script's variables to define what version/arch/binaries you want to download.</p> <p>Set up up your Red Hat pull-secret before using the script if you intend to extract the <code>openshift-installer</code> as it will need valid credentials for access.</p> <p><code>wget https://raw.githubusercontent.com/yojoshb/disco-docs/refs/heads/main/_scripts/rhel-oc-tools.sh</code></p> </li> <li> <p>General Release Information</p> </li> </ul> <ul> <li>oc: The OpenShift Client command line tool to interact with the cluster, also needed to use CLI plugins.<ul> <li>Red Hat Docs</li> <li>You can use the latest-stable version available for your architecture. <code>oc</code> newer than your cluster version may have additional capabilities that your cluster cannot use.</li> <li>RHEL 9 latest-stable: <code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-client-linux-amd64-rhel9.tar.gz</code></li> <li>RHEL 8 latest-stable: <code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-client-linux-amd64-rhel8.tar.gz</code></li> </ul> </li> </ul> <ul> <li>oc-mirror: Awesome oc cli plugin to streamline getting the required images mirrored and packed into a <code>.tar</code> file to transfer to the high-side. Also used to upload the images into your mirror registry on the high-side once they are brought over.<ul> <li>Red Hat Docs</li> <li>Always use the latest version available for your architecture.</li> <li>oc-mirror v2 is new, and GA'd for OpenShift 4.18, but is backwards compatible with older releases down to v4.12.</li> <li>RHEL 9 latest: <code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/latest/oc-mirror.rhel9.tar.gz</code></li> <li>RHEL 8 latest: <code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/latest/oc-mirror.tar.gz</code></li> </ul> </li> </ul> <ul> <li> <p>openshift-install: Program that will create the OpenShift install disk that will bootstrap and install the cluster on your hardware. </p> <p>Important</p> <p>This binary is specific to the release version of OpenShift you are installing. The binary must match the release images that you mirror.</p> <ul> <li>This doc will go over the extraction method to make sure the correct binary is downloaded for the release images mirrored. </li> </ul> </li> <li> <p>mirror-registry (optional): Small registry that can host the required container images to install, update, and maintain the cluster.</p> <ul> <li>Red Hat Docs</li> <li>Always use the latest version available for your architecture.</li> <li><code>wget https://mirror.openshift.com/pub/cgw/mirror-registry/latest/mirror-registry-amd64.tar.gz</code></li> </ul> </li> </ul> <ul> <li>butane: CLI tool to create machine config files to customize OpenShift nodes in your environment. <ul> <li>Red Hat Docs</li> <li>Always use the latest version available for your architecture.</li> <li><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/butane/latest/butane-amd64</code></li> </ul> </li> </ul>"},{"location":"connected/tools.html#grab-your-pull-secret-from-your-red-hat-account","title":"Grab your pull-secret from your Red Hat Account","text":"<p>Red Hat Docs</p> <ol> <li> <p>Download or copy your pull secret from the Red Hat OpenShift Console. These are your credentials for accessing Red Hat container registries.</p> </li> <li> <p>Make a copy of your pull secret in readable JSON format: <pre><code>cat ./pull-secret | jq . &gt; rh-pull-secret.json\n</code></pre></p> </li> <li> <p>Specify the path to the folder to store the pull secret in and a name for the JSON file that you create. You can store this file in <code>/home/$USER/.docker/config.json</code> or <code>$XDG_RUNTIME_DIR/containers/auth.json</code>. If one of the directories aren't there, create them.</p> <ul> <li>The contents of the file resemble the following example: $XDG_RUNTIME_DIR/containers/auth.json<pre><code>{\n  \"auths\": {\n    \"cloud.openshift.com\": {\n      \"auth\": \"b3BlbnNo...\",\n      \"email\": \"you@example.com\"\n    },\n    \"quay.io\": {\n      \"auth\": \"b3BlbnNo...\",\n      \"email\": \"you@example.com\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"NTE3Njg5Nj...\",\n      \"email\": \"you@example.com\"\n    },\n    \"registry.redhat.io\": {\n      \"auth\": \"NTE3Njg5Nj...\",\n      \"email\": \"you@example.com\"\n    }\n  }\n}\n</code></pre></li> </ul> </li> <li> <p>Verify you can authenticate to <code>registry.redhat.io</code> <pre><code>podman login registry.redhat.io\n</code></pre> Example Output<pre><code>Authenticating with existing credentials for registry.redhat.io\nExisting credentials are valid. Already logged in to registry.redhat.io\n</code></pre></p> </li> </ol>"},{"location":"connected/tools.html#installconfigure-tools","title":"Install/configure tools","text":"<ol> <li>Put <code>oc</code> and <code>oc-mirror</code> tools we need on the low-side in your <code>$PATH</code>. Either <code>/usr/local/bin</code> or somewhere like <code>/home/$USER/bin</code> or <code>/home/$USER/.local/bin</code> <pre><code>sudo tar -xzvf openshift-client-linux.tar.gz -C /usr/local/bin/\n\n# RHEL 8\nsudo tar -xzvf oc-mirror.tar.gz -C /usr/local/bin/\n\n# RHEL 9\nsudo tar -xzvf oc-mirror.rhel9.tar.gz -C /usr/local/bin/\n\nsudo chmod +x /usr/local/bin/{oc,oc-mirror}\n\n# If selinux is enabled\nsudo restorecon -v /usr/local/bin/{oc,oc-mirror}\n</code></pre></li> <li> <p>Make sure you have set the umask parameter to <code>0022</code> on the operating system that uses oc-mirror <pre><code>umask 0022\n</code></pre></p> </li> <li> <p>Verify that the oc-mirror v2 plugin is successfully installed by running the following command <pre><code>oc mirror --v2 --help\n</code></pre></p> </li> </ol> <p>Info</p> <p>If the system is STIG'd and using fapolicyd either disable it, or make changes as it automatically blocks any binary that is not an RPM.</p> <p>You can add the binaries to the policy like so:</p> <pre><code>systemctl stop fapolicyd.service\n\nsudo fapolicyd-cli --file add /usr/local/bin/oc\nsudo fapolicyd-cli --file add /usr/local/bin/oc-mirror\nsudo fapolicyd-cli --update\n\nsystemctl start fapolicyd.service; systemctl status fapolicyd.service\n</code></pre>"},{"location":"disconnected/index.html","title":"Disconnected Setup","text":"<p>Red Hat Docs</p> <p>The disconnected Linux machine will need to be able to run the tools and have adequate disk space on the mirror host as well as the target registry you will be uploading to.</p> <p>Note</p> <p>This document goes over the installation using the builtin HAProxy load-balancer that OpenShift comes with out of the box.</p> <p>This is not a true load balancer as traffic will always go to the pod where Ingress VIP is attached.</p> <p>If you want to use an external load-balancer (encouraged for Production), refer to the Red Hat Docs</p> <p>The general flow of goes like this: </p> <ol> <li>Copy the image set and associated tools off of the transfer medium to the disconnected environment.</li> <li>Configure push/pull credentials for your target mirror registry.</li> <li>Upload the image set to the target mirror registry.</li> </ol>"},{"location":"disconnected/data.html","title":"2.1 Tools and data","text":""},{"location":"disconnected/data.html#prepare-the-tools","title":"Prepare the tools","text":"<p>Identify a host on the disconnected network that will be used for installing the cluster. Optionally, identify a host that will become the mirror registry if you are using the Red Hat Mirror Registry and do not already have a registry set up in your environment.</p> <ul> <li> <p>If you brought the tools over as <code>.tar</code> extract them to your <code>$PATH</code> like the connected process, or copy them there if you brought the binaries to your high-side host <pre><code>sudo cp /mnt/transfer-disk/{oc,oc-mirror,butane} /usr/local/bin/\nsudo chmod +x /usr/local/bin/{oc,oc-mirror,butane}\n\n# If selinux is enabled\nsudo restorecon -v /usr/local/bin/{oc,oc-mirror,butane}\n</code></pre></p> </li> <li> <p>Make sure you have set the umask parameter to <code>0022</code> on the operating system that uses oc-mirror <pre><code>umask 0022\n</code></pre></p> </li> <li> <p>Verify oc mirror works <pre><code>oc mirror --v2 --help\n</code></pre></p> </li> <li> <p>If you brought over the <code>openshift-install</code> binary copy it to your <code>$PATH</code>. <pre><code>sudo cp /mnt/transfer-disk/openshift-install /usr/local/bin/\nsudo chmod +x /usr/local/bin/openshift-install\n\n# If selinux is enabled\nsudo restorecon -v /usr/local/bin/openshift-install\n</code></pre></p> </li> <li> <p>Optional: Copy the mirror-registry-amd64.tar.gz file to the host that you want to become your mirror registry. This can be the same host, just make sure you have enough storage space to hold the mirrored images that will be uploaded into the registry <pre><code>cp /mnt/transfer-disk/mirror-registry-amd64.tar.gz /opt\n</code></pre></p> </li> </ul> <p>Info</p> <p>If the system is STIG'd and using fapolicyd either disable it, or make changes as it automatically blocks any binary that is not an RPM.</p> <p>You can add the binaries to the policy like so:</p> <pre><code>systemctl stop fapolicyd.service\n\nsudo fapolicyd-cli --file add /usr/local/bin/oc\nsudo fapolicyd-cli --file add /usr/local/bin/oc-mirror\nsudo fapolicyd-cli --file add /usr/local/bin/butane\nsudo fapolicyd-cli --file add /usr/local/bin/openshift-install\nsudo fapolicyd-cli --update\n\nsystemctl start fapolicyd.service; systemctl status fapolicyd.service\n</code></pre>"},{"location":"disconnected/data.html#create-a-directory-structure","title":"Create a directory structure","text":"<p>Note</p> <p>You can just keep everything on the <code>transfer-disk</code> and mirror off of it if you want and skip this step. Be sure to stay organized</p> <ol> <li> <p>Do this how you see fit for your environment. Identify a space on your disconnected machine that can hold the imageset-config.yaml, mirror_000001.tar, and generated cluster configs <pre><code>mkdir /opt/4.17-mirrordata\n</code></pre></p> </li> <li> <p>Copy the imageset-config and mirror_000001.tar to that directory <pre><code>cp /mnt/transfer-disk/{imageset-config.yaml,mirror_000001.tar} /opt/4.17-mirrordata/\n</code></pre></p> </li> </ol>"},{"location":"disconnected/data.html#create-your-pullpush-secret-for-your-mirror-registry","title":"Create your pull/push secret for your mirror registry","text":"<p>If you already have a registry in your target environment, you can generate a secret from it and place it in a json file like earlier. </p> <p>If you do not have a registry in your target environment that can store the mirror images, install the Red Hat Mirror Registry.</p> <ol> <li> <p>Make a copy of your pull secret in JSON format:     <pre><code>cat ./pull-secret | jq . &gt; registry-pull-secret.json\n</code></pre></p> </li> <li> <p>Specify the path to the folder to store the pull secret in and a name for the JSON file that you create. You can store this file in <code>/home/$USER/.docker/config.json</code> or <code>$XDG_RUNTIME_DIR/containers/auth.json</code>. If one of the directories aren't there, create them.</p> <ul> <li>The contents of the file resemble the following example: $XDG_RUNTIME_DIR/containers/auth.json<pre><code>{\n  \"auths\": {\n    \"registry.example.com:8443\": {\n      \"auth\": \"b3BlbnNo...\",\n      \"email\": \"you@example.com\"\n    }\n  }\n}\n</code></pre></li> </ul> </li> <li> <p>Verify that you can login to your registry. Your account should have push and pull permissions to your registry     <pre><code>podman login --tls-verify=false registry.example.com:8443\n</code></pre> Example Output<pre><code>Authenticating with existing credentials for registry.example.com:8443\nExisting credentials are valid. Already logged in to registry.example.com:8443\n</code></pre></p> </li> <li> <p>By the end, you should have a Registry account that can push/pull (so oc-mirror can push images to it) and a account that can only pull (so the cluster can access the images for installing/updating). This registry should ideally only be used to hold OpenShift release images.</p> </li> </ol>"},{"location":"disconnected/dns.html","title":"2.3 DNS Records","text":""},{"location":"disconnected/dns.html#create-dns-records","title":"Create DNS records","text":"<p>Red Hat Docs</p> <p>These records are mandatory for the cluster to function. Technically the cluster can function without an external DNS source, but it will be much harder to use and access the cluster and it's resources. Each host wanting to interact with the cluster will need a curated <code>/etc/hosts</code> file provisioned for every endpoint on the cluster.</p> <ul> <li>The following DNS records are required for a user-provisioned OpenShift Container Platform cluster and they must be in place before installation. In each record, <code>&lt;cluster_name&gt;</code> is the cluster name and <code>&lt;base_domain&gt;</code> is the base domain that you specify in the install-config.yaml file. A complete DNS record takes the form: <code>&lt;component&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></li> </ul> Component Record Type Description Kubernetes API api.&lt;cluster_name&gt;.&lt;base_domain&gt;. DNS A/AAAA or CNAME and DNS PTR record To identify the API load balancer. These records must be resolvable by both clients external to the cluster and from all the nodes within the cluster. Kubernetes API api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;. DNS A/AAAA or CNAME and DNS PTR record To internally identify the API load balancer. These records must be resolvable from all the nodes within the cluster. Routes *.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;. wildcard DNS A/AAAA or CNAME record Refers to the application ingress load balancer. The application ingress load balancer targets the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default. These records must be resolvable by both clients external to the cluster and from all the nodes within the cluster. Control plane machines &lt;control_plane&gt;&lt;n&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;. DNS A/AAAA or CNAME and DNS PTR record To identify each machine for the control plane nodes. These records must be resolvable by the nodes within the cluster. Compute machines &lt;compute&gt;&lt;n&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;. DNS A/AAAA or CNAME and DNS PTR record To identify each machine for the worker nodes. These records must be resolvable by the nodes within the cluster. <ul> <li>Here's and example for a 3 node high-avalablity compact cluster. The cluster name is <code>cluster</code> and the base domain is <code>example.com</code>. The <code>registry.example.com</code> record is displayed here as well, showing that it's necessary to be DNS resolvable by your OpenShift nodes on your network. The registry does not have to belong to the cluster sub-domain. </li> </ul> Component Record IP Address Type Kubernetes API api.cluster.example.com 172.16.1.5 DNS A/PTR record Kubernetes API api-int.cluster.example.com 172.16.1.5 DNS A/PTR record Routes *.apps.cluster.example.com 172.16.1.6 DNS wildcard record Master node 1 m1.cluster.example.com 172.16.1.10 DNS A/PTR record Master node 2 m2.cluster.example.com 172.16.1.11 DNS A/PTR record Master node 3 m3.cluster.example.com 172.16.1.12 DNS A/PTR record Registry registry.example.com 172.16.1.1 DNS A/PTR record <ul> <li>For Single Node OpenShift (SNO) the Kubernetes API and Routes all must point to the SNO node itself if you aren't using a external load-balancer. The cluster name is <code>sno</code> and the base domain is <code>example.com</code>.</li> </ul> Component Record IP Address Type Kubernetes API api.sno.example.com 172.16.1.10 DNS A/PTR record Kubernetes API api-int.sno.example.com 172.16.1.10 DNS A/PTR record Routes *.apps.sno.example.com 172.16.1.10 DNS wildcard record Single node 1 n1.sno.example.com 172.16.1.10 DNS A/PTR record Registry registry.example.com 172.16.1.1 DNS A/PTR record <ul> <li>Check forward, reverse, and wildcard DNS resolution<ul> <li>Forward lookup for the record <code>api.cluster.example.com</code> answered by the DNS server at <code>172.16.1.254</code> <pre><code>dig api.cluster.example.com\n</code></pre> Example Output<pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.23-RH &lt;&lt;&gt;&gt; api.cluster.example.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 13520\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;api.cluster.example.com.           IN      A\n\n;; ANSWER SECTION:\napi.cluster.example.com.    3600    IN      A       172.16.1.5\n\n;; Query time: 0 msec\n;; SERVER: 172.16.1.254#53(172.16.1.254)\n;; WHEN: Mon Mar 24 16:11:11 CDT 2025\n;; MSG SIZE  rcvd: 64\n</code></pre></li> <li>Reverse lookup for the record <code>172.16.1.5</code> answered by the DNS server at <code>172.16.1.254</code> <pre><code>dig -x 172.16.1.5\n</code></pre> Example Output<pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.23-RH &lt;&lt;&gt;&gt; -x 172.16.1.5\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 62615\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;5.1.16.172.in-addr.arpa.     IN      PTR\n\n;; ANSWER SECTION:\n5.1.16.172.in-addr.arpa. 3600 IN      PTR     api.cluster.example.com.\n\n;; Query time: 0 msec\n;; SERVER: 172.16.1.254#53(172.16.1.254)\n;; WHEN: Mon Mar 24 16:11:52 CDT 2025\n;; MSG SIZE  rcvd: 91 \n</code></pre></li> <li>Wildcard lookup for the record <code>someapp.apps.cluster.example.com</code> answered by the DNS server at <code>172.16.1.254</code> <pre><code>dig someapp.apps.cluster.example.com\n</code></pre> Example Output<pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.23-RH &lt;&lt;&gt;&gt; someapp.apps.cluster.example.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 46996\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;someapp.apps.cluster.example.com.  IN      A\n\n;; ANSWER SECTION:\nsomeapp.apps.cluster.example.com. 3600 IN   A       172.16.1.6\n\n;; Query time: 0 msec\n;; SERVER: 172.16.1.254#53(172.16.1.254)\n;; WHEN: Mon Mar 24 16:13:18 CDT 2025\n;; MSG SIZE  rcvd: 73 \n</code></pre></li> </ul> </li> </ul>"},{"location":"disconnected/mirroring.html","title":"2.2 Mirror images to registry","text":""},{"location":"disconnected/mirroring.html#mirroring-images-from-disk-to-your-mirror","title":"Mirroring images from disk to your mirror","text":"<p>Red Hat Docs</p> <p>Now that the images are on the disk and you have a target registry with push/pull permissions to mirror to, we can mirror them to your registry</p> <ul> <li>Specify the image set configuration file that you brought over or created. This example assumes that it is in <code>/opt/4.17-mirrordata/imageset-config.yaml</code>. If you named your's differently, sub in your name and path of the file.</li> <li>Specify the target directory where the <code>mirror_000001.tar</code> file is. The target directory path must start with <code>file://</code>. This procedure assumes you want to upload the mirror_000001.tar from <code>/opt/4.17-mirrordata/</code>.<ul> <li>The target directory will also hold the <code>working-dir</code> environment. This directory contains the various necessary data to build, update, and maintain cluster resources. Keep this directory safe, and do not modify it. It will be used again for updates and additions to your cluster</li> </ul> </li> <li>Specify the registry you will be mirroring the images to. In this example <code>registry.example.com:8443/</code> is our registry, and we will upload it to the <code>ocp</code> namespace in our registry.</li> <li>Be aware of the caching system, this will also take up considerable space on the disk depending on how many images are being uploaded to your mirror. Caching still occurs with the 'disk to mirror' workflow.</li> </ul> <p>Caching</p> <ul> <li>How does the cache work?<ul> <li>It's like a local registry, it can take up additional disk space almost as large as the .tar that gets generated</li> </ul> </li> <li>Where is it saved?<ul> <li>By default in <code>$HOME/.oc-mirror/.cache</code></li> </ul> </li> <li>Can I control where I want the cache to be stored?<ul> <li>Yes, you can pass <code>--cache-dir &lt;dir&gt;</code> which will change the cache location to <code>&lt;dir&gt;/.oc-mirror/.cache</code></li> </ul> </li> <li>During the mirroring process, is there a way to resume if something goes wrong?<ul> <li>Yes, by re-running oc mirror</li> </ul> </li> <li>I intentionally canceled the task and re-ran the mirroring process, but it seemed to start from the beginning.<ul> <li>It goes through the images from your ISC but it won't pull them from the tarball if they're already in the cache. You can compare the elapsed times by running a second time with the images already cached.</li> </ul> </li> <li>The cache takes up a lot of disk space can it be deleted?<ul> <li>Yes the cache can be removed, oc mirror will just re-pull what's needed from the tarball</li> </ul> </li> </ul> <ol> <li> <p>You have set the umask parameter to <code>0022</code> on the operating system that uses oc-mirror.     <pre><code>umask 0022\n</code></pre></p> </li> <li> <p>Upload your images to your mirror   <pre><code>oc mirror -c /opt/4.17-mirrordata/imageset-config.yaml --from file:///opt/4.17-mirrordata docker://registry.example.com:8443/ocp --v2\n</code></pre>   If your mirror registry is using a self-signed certificate and your machine doesn't trust it internally use <code>--dest-tls-verify=false</code> <pre><code>oc mirror --dest-tls-verify=false -c /opt/4.17-mirrordata/imageset-config.yaml --from file:///opt/4.17-mirrordata docker://registry.example.com:8443/ocp --v2\n</code></pre> Example Output<pre><code>...\n[INFO]   : === Results ===\n[INFO]   :  \u2713  185 / 185 release images mirrored successfully\n[INFO]   :  \u2713  8 / 8 operator images mirrored successfully\n[INFO]   :  \u2713  1 / 1 additional images mirrored successfully\n[INFO]   : \ud83d\udcc4 Generating IDMS file...\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml file created\n[INFO]   : \ud83d\udcc4 Generating ITMS file...\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/itms-oc-mirror.yaml file created\n[INFO]   : \ud83d\udcc4 Generating CatalogSource file...\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/cs-redhat-operator-index-v4-17.yaml file created\n[INFO]   : \ud83d\udcc4 Generating ClusterCatalog file...\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/cc-redhat-operator-index-v4-17.yaml file created\n[INFO]   : \ud83d\udcc4 Generating Signature Configmap...\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/signature-configmap.json file created\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/signature-configmap.yaml file created\n[INFO]   : \ud83d\udcc4 Generating UpdateService file...\n[INFO]   : /opt/4.17-mirrordata/working-dir/cluster-resources/updateService.yaml file created\n[INFO]   : mirror time     : 14m20.563306852s\n[INFO]   : \ud83d\udc4b Goodbye, thank you for using oc-mirror\n</code></pre></p> <ul> <li>If you get an error like the one below, your registry most likely cannot write the data fast enough. You can try doing it again with less parallel operations using the <code>--parallel-images</code> flag Example Output<pre><code>[ERROR]  : [Worker] error mirroring image quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dae0aadc59c79509779a9e904e0aeaa6d5c5e3f24eedc5c114c6bca1b15ea3b1 error: trying to reuse blob sha256:25c75c34b2e2b68ba9245d9cddeb6b8a0887371ed30744064f85241a75704d87 at destination: can't talk to a V1 container registry\n</code></pre> <pre><code># Default is 8 parallel operations, so you can pull it down to a lower value\noc mirror --dest-tls-verify=false --parallel-images 4 -c /opt/4.17-mirrordata/imageset-config.yaml --from file:///opt/4.17-mirrordata docker://registry.example.com:8443/ocp --v2\n</code></pre></li> </ul> </li> <li> <p>Verify the cluster resources were generated by oc mirror in the <code>working-dir/cluster-resources</code> directory, these resources will be applied to the cluster later once it's installed     <pre><code>ls /opt/4.17-mirrordata/working-dir/cluster-resources/\n</code></pre> Example Output<pre><code>cc-redhat-operator-index-v4-17.yaml  cs-redhat-operator-index-v4-17.yaml  idms-oc-mirror.yaml  itms-oc-mirror.yaml  signature-configmap.json  signature-configmap.yaml  updateService.yaml\n</code></pre></p> </li> </ol>"},{"location":"examples/agent-configs.html","title":"agent-config.yaml","text":""},{"location":"examples/agent-configs.html#examples-of-agent-configyaml-files","title":"Examples of agent-config.yaml files","text":"<p>Red Hat Docs</p> <p>Red Hat Docs: Agent Configuration Parameters</p> <p>Bill's awesome collection of agent installation examples</p> <p>Various examples of common agent-configs. Sub in your data as appropriate.</p> <p>Important</p> <p>For each host you configure, you must provide the MAC address of an interface on the host, the name of the interface can be whatever you want as long as you know the MAC. </p> <p>You can configure additional interfaces on your hosts after the cluster is installed by utilizing the <code>NMState Operator</code>.</p>"},{"location":"examples/agent-configs.html#bondslink-aggregation","title":"Bonds/Link Aggregation","text":"<pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: cluster\nrendezvousIP: 172.16.10.10\nadditionalNTPSources:\n- ntp.example.com\nhosts:\n- hostname: master1.cluster.example.com\n  role: master\n  interfaces:\n  - name: eno1\n    macAddress: 00:ef:44:21:e6:a1\n  - name: eno2\n    macAddress: 00:ef:44:21:e6:a2\n  networkConfig:\n    interfaces:\n    - name: bond0\n      description: Access mode bond using ports eno1 and eno2\n      type: bond\n      state: up\n      ipv4:\n        enabled: true\n        address:\n        - ip: 172.16.10.10\n          prefix-length: 24\n        dhcp: false\n      link-aggregation:\n        mode: 802.3ad  # mode=1 active-backup, mode=2 balance-xor or mode=4 802.3ad\n        options:\n          miimon: '150'\n        port:\n        - eno1\n        - eno2\n\n    dns-resolver:\n      config:\n        server:\n        - 172.16.10.1\n    routes:\n      config:\n      - destination: 0.0.0.0/0\n        next-hop-address: 172.16.10.254\n        next-hop-interface: bond0\n        table-id: 254\n</code></pre>"},{"location":"examples/agent-configs.html#vlans-bondslink-aggregation","title":"VLANs &amp; Bonds/Link Aggregation","text":"<p>Red Hat Docs</p> <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: cluster\nrendezvousIP: 172.16.10.10\nadditionalNTPSources:\n- ntp.example.com\nhosts:\n- hostname: master1.cluster.example.com\n  role: master\n  interfaces:\n  - name: eno1\n    macAddress: 00:ef:44:21:e6:a1\n  - name: eno2\n    macAddress: 00:ef:44:21:e6:a2\n  networkConfig:\n    interfaces:\n    - name: eno1\n      type: ethernet\n      state: up\n    - name: eno2\n      type: ethernet\n      state: up\n\n    - name: bond0\n      description: Trunk mode bond using ports eno1 and eno2\n      type: bond\n      state: up\n      ipv4:\n        enabled: false\n      ipv6:\n        enabled: false\n      link-aggregation:\n        mode: 802.3ad  # mode=1 active-backup, mode=2 balance-xor or mode=4 802.3ad\n        options:\n          miimon: '150'\n        port:\n        - eno1\n        - eno2\n\n    - name: bond0.100  # Example VLAN 100 on the 'main' network subnet\n      type: vlan\n      state: up\n      vlan:\n        base-iface: bond0\n        id: 100\n      ipv4:\n        enabled: true\n        address:\n        - ip: 172.16.10.10\n          prefix-length: 24\n        dhcp: false\n      ipv6:\n        enabled: false\n\n    - name: bond0.500  # Example VLAN 500 on different subnet\n      type: vlan\n      state: up\n      vlan:\n        base-iface: bond0\n        id: 500\n      ipv4:\n        enabled: true\n        address:\n        - ip: 172.16.50.10\n          prefix-length: 24\n        dhcp: false\n      ipv6:\n        enabled: false\n\n    dns-resolver:\n      config:\n        server:\n        - 172.16.10.1\n    routes:\n      config:\n      - destination: 0.0.0.0/0\n        next-hop-address: 172.16.10.254\n        next-hop-interface: bond0.100\n        table-id: 254\n</code></pre>"},{"location":"examples/agent-configs.html#root-device-hints","title":"Root Device Hints","text":"<p>The <code>rootDeviceHints</code> parameter enables the installer to provision the Red Hat Enterprise Linux CoreOS (RHCOS) image to a particular device. The installer examines the devices in the order it discovers them, and compares the discovered values with the hint values. The installer uses the first discovered device that matches the hint value. The configuration can combine multiple hints, but a device must match all hints for the installer to select it.</p> <p>Info</p> <p>By default, <code>/dev/sda</code> path is used when no hints are specified. Furthermore, Linux does not guarantee the block device names to be consistent across reboots.</p> Subfield Description <code>deviceName</code> A string containing a Linux device name such as <code>/dev/vda</code> or <code>/dev/disk/by-path/</code>. It is recommended to use the <code>/dev/disk/by-path/&lt;device_path&gt;</code> link to the storage location. The hint must match the actual value exactly. <code>hctl</code> A string containing a SCSI bus address like <code>0:0:0:0</code>. The hint must match the actual value exactly. <code>model</code> A string containing a vendor-specific device identifier. The hint can be a substring of the actual value. <code>vendor</code> A string containing the name of the vendor or manufacturer of the device. The hint can be a sub-string of the actual value. <code>serialNumber</code> A string containing the device serial number. The hint must match the actual value exactly. <code>minSizeGigabytes</code> An integer representing the minimum size of the device in gigabytes. <code>wwn</code> A string containing the unique storage identifier. The hint must match the actual value exactly. If you use the <code>udevadm</code> command to retrieve the <code>wwn</code> value, and the command outputs a value for <code>ID_WWN_WITH_EXTENSION</code>, then you must use this value to specify the <code>wwn</code> subfield. <code>rotational</code> A boolean indicating whether the device must be a rotating disk (true) or not (false). Examples of non-rotational devices include SSD and NVMe storage. <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: cluster\nrendezvousIP: 172.16.10.10\nadditionalNTPSources:\n- ntp.example.com\nhosts:\n- hostname: master1.cluster.example.com\n  role: master\n  rootDeviceHints:\n    deviceName: \"/dev/disk/by-path/pci-0000:01:00.0-nvme-1\"\n  interfaces:\n   - name: eno1\n     macAddress: 00:ef:44:21:e6:a1\n  networkConfig:\n      interfaces:\n      - name: enp6s18\n        type: ethernet\n        state: up\n        mac-address: BC:24:11:EE:DD:C1\n        ipv4:\n          enabled: true\n          address:\n          - ip: 172.16.1.10\n            prefix-length: 24\n          dhcp: false\n      dns-resolver:\n        config:\n          server:\n          - 172.16.1.254\n      routes:\n        config:\n        - destination: 0.0.0.0/0\n          next-hop-address: 172.16.1.254\n          next-hop-interface: enp6s18\n          table-id: 254\n</code></pre>"},{"location":"examples/agent-configs.html#dhcp","title":"DHCP","text":"<p>If you want to utilize DHCP, the config is very simple and you can leave out the <code>networkConfig</code> enitirely. It's a good idea to at least create a static reservation for the <code>rendezvousIP</code> and master nodes if you want them to be on specific machines. The <code>rendezvousIP</code> must be a master node. You can source NTP directly from DHCP if your DHCP server has this option</p> <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: cluster\nrendezvousIP: 172.16.10.10\n\n# You can omit this section below entirely if you wanna let DHCP just assign addressing at random\nhosts:\n- hostname: master1.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C1\n\n- hostname: master2.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C2\n\n- hostname: master3.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C3\n</code></pre> <p>(Opinionated): It may be better represented to still explicitly define the interfaces like so</p> <pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: cluster\nrendezvousIP: 172.16.10.10\n\nhosts:\n- hostname: master1.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C1\n  networkConfig:\n    interfaces:\n    - name: enp6s18\n      type: ethernet\n      state: up\n      ipv4:\n        enabled: true\n        dhcp: true\n      ipv6:\n        enabled: false\n\n- hostname: master2.cluster.example.com\n  role: master\n  interfaces:\n    - name: enp6s18\n      macAddress: BC:24:11:EE:DD:C2\n    networkConfig:\n      interfaces:\n      - name: enp6s18\n        type: ethernet\n        state: up\n        ipv4:\n          enabled: true\n          dhcp: true\n        ipv6:\n          enabled: false\n\n- hostname: master3.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C3\n  networkConfig:\n    interfaces:\n    - name: enp6s18\n      type: ethernet\n      state: up\n      ipv4:\n        enabled: true\n        dhcp: true\n      ipv6:\n        enabled: false\n</code></pre>"},{"location":"examples/imageset-configs.html","title":"imageset-config.yaml","text":""},{"location":"examples/imageset-configs.html#examples-of-imageset-configyaml-files","title":"Examples of imageset-config.yaml files","text":"<p>Various examples of common imageset-configs (ISC) including Operators. Sub in your data as appropriate.</p> <p>Important</p> <p>Operators and catalogs are version specific. Be sure to prune through the catalogs so you mirror the correct version of the operators for your target platform</p> <p>Feel free to mix, match, and create different imagesets for different deployments</p> <p>Red Hat Blog about Operators in disconnected environments</p> <p>To prune through the various catalogs, use the following commands</p> <ul> <li> <p>OpenShift Releases <pre><code># Output OpenShift release versions\noc mirror list releases\n\n# Output all OpenShift release channels list for a release\noc mirror list releases --version=4.17\n\n# List all OpenShift versions in a specified channel\noc mirror list releases --channel=stable-4.17\n\n# List all OpenShift channels for a specific version\noc mirror list releases --channels --version=4.17\n\n# List OpenShift channels for a specific version and one or more release architecture. \n# Valid architectures: amd64 (default), arm64, ppc64le, s390x, multi.\noc mirror list releases --channels --version=4.17 --filter-by-archs amd64,arm64,ppc64le,s390x,multi\n</code></pre></p> </li> <li> <p>Operators</p> <p>Tip</p> <p>This handy little one-liner will dump all available operators and their corresponding catalogs and store them in text files for a specific version. This will take some time to run initially, but saves a bunch of time in the long run</p> <p>Source: Allens Repository with more helpful scripts and configs</p> <pre><code>for i in $(oc-mirror list operators --catalogs --version=4.17 | grep registry); do $(oc-mirror list operators --catalog=$i --version=4.17 &gt; $(echo $i | cut -b 27- | rev | cut -b 7- | rev).txt); done\n</code></pre> </li> </ul> <pre><code># List available operator catalog release versions\noc mirror list operators\n\n# Output default operator catalogs for OpenShift release 4.17\noc mirror list operators --catalogs --version=4.17\n\n# List all operator packages in a catalog\noc mirror list operators --catalog=catalog-name\n\n# List all channels in an operator package\noc mirror list operators --catalog=catalog-name --package=package-name\n\n# List all available versions for a specified operator in a channel\noc mirror list operators --catalog=catalog-name --package=operator-name --channel=channel-name\n</code></pre>"},{"location":"examples/imageset-configs.html#openshift-versioning","title":"OpenShift Versioning","text":"<ul> <li>You can limit the min and max version of platform releases you want to pull</li> </ul> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  platform:\n    architectures:\n    - amd64\n    channels:\n    - type: ocp\n      name: stable-4.17\n      minVersion: 4.17.17\n      maxVersion: 4.17.21\n</code></pre> <ul> <li>You can also pin to a specific version</li> </ul> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  platform:\n    architectures:\n    - amd64\n    channels:\n    - type: ocp\n      name: stable-4.17\n      minVersion: 4.17.21\n      maxVersion: 4.17.21\n</code></pre> <ul> <li>For other architectures, you can select the filter for the target platform </li> </ul> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  platform:\n    architectures:\n    - arm64    # Valid architectures: amd64, arm64, ppc64le, s390x, multi\n    channels:\n    - type: ocp\n      name: stable-4.17\n</code></pre>"},{"location":"examples/imageset-configs.html#openshift-update-service-osus","title":"OpenShift Update Service (OSUS)","text":"<ul> <li>Specify your release and set <code>graph: true</code>. The graph data along with the <code>cincinnati-operator</code> allows you to use the same update mechanism that a connected cluster would use. <ul> <li>To install and configure OSUS instructions are here in this document</li> </ul> </li> </ul> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  platform:\n    architectures:\n    - amd64\n    channels:\n    - type: ocp\n      name: stable-4.17\n    graph: true\n\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17\n    packages:\n    - name: cincinnati-operator\n</code></pre>"},{"location":"examples/imageset-configs.html#ansible-automation-platform","title":"Ansible Automation Platform","text":"<p>The Operator will not contain any collections or execution/decision environments. So if you want to utilize the Automation Hub on the cluster, you'll want to pull them over to the disconnected network. It's a bit tedious at the moment, hopefully this will be easier in the future. </p> <ul> <li> <p>To get the collections, go to Automation Hub, and download the tarballs of the collections that you would like to include in your Private Automation Hub.</p> </li> <li> <p>Follow the old 2.3 docs to add the collections you downloaded and transfered. </p> </li> </ul> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17\n    packages:\n\n    # Ansible Automation Platform Operator\n    - name: ansible-automation-platform-operator\n      channels:\n      - name: stable-2.5\n\n  additionalImages:\n  # Images that the Containerized/RPM offline-bundle comes with. You can bring these over independentaly with podman also. Make sure to point PAH to your registry where these are mirrored to.\n  - name: registry.redhat.io/ansible-automation-platform-25/ee-supported-rhel8:latest\n  - name: registry.redhat.io/ansible-automation-platform-25/ee-minimal-rhel8:latest\n  - name: registry.redhat.io/ansible-automation-platform-25/de-supported-rhel8:latest\n</code></pre>"},{"location":"examples/imageset-configs.html#openshift-data-foundation-odf-operators","title":"OpenShift Data Foundation (ODF) Operators","text":"<p>Red Hat ODF 4.17 Docs</p> <p>Consult the docs for whatever version of ODF you are wanting to install. The operators required vary per OCP version.</p> ODF 4.17<pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17\n    packages:\n    - name: ocs-operator\n      channels:\n      - name: stable-4.17\n    - name: odf-operator\n      channels:\n      - name: stable-4.17\n    - name: mcg-operator\n      channels:\n      - name: stable-4.17\n    - name: odf-csi-addons-operator\n      channels:\n      - name: stable-4.17\n    - name: ocs-client-operator\n      channels:\n      - name: stable-4.17\n    - name: odf-prometheus-operator\n      channels:\n      - name: stable-4.17\n    - name: recipe\n      channels:\n      - name: stable-4.17\n    - name: rook-ceph-operator\n      channels:\n      - name: stable-4.17\n    - name: cephcsi-operator\n      channels:\n      - name: stable-4.17\n    - name: odr-cluster-operator\n      channels:\n      - name: stable-4.17\n    - name: odr-hub-operator\n      channels:\n      - name: stable-4.17\n\n    # For local storage deployments i.e. node attached disks \n    - name: local-storage-operator\n      channels:\n      - name: stable\n\n    # Optional: Only for Regional Disaster Recovery (Regional-DR) configuration\n    - name: odf-multicluster-orchestrator\n      channels:\n      - name: stable-4.17\n\n  additionalImages:\n  # Optional: ODF Must gather support tools\n  - name: registry.redhat.io/odf4/odf-must-gather-rhel9:v4.17\n</code></pre>"},{"location":"examples/imageset-configs.html#openshift-virtulization-kubevirt-and-migration-kit-for-virtualization-mtv-operators","title":"OpenShift Virtulization (KubeVirt) and Migration Kit for Virtualization (MTV) Operators","text":"<p>KubeVirt</p> <p>MTV</p> <p>VDDK</p> <p>Prebuilt VDDK Image</p> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17\n    packages:\n    # Virtualization\n    - name: kubevirt-hyperconverged\n      channels:\n      - name: stable\n    - name: kubernetes-nmstate-operator\n      channels:\n      - name: stable\n\n    # Optional: If you would like to use local dynamic storage on another disk attached to the host\n    - name: lvms-operator\n      defaultChannel: stable-4.17\n      channels:\n      - name: stable-4.17\n\n    # Migration Toolkit for Virtulization\n    - name: mtv-operator\n      channels:\n      - name: release-v2.6\n\n  additionalImages:\n  # Optional: Virtual guest images\n  - name: registry.redhat.io/rhel8/rhel-guest-image:latest\n  - name: registry.redhat.io/rhel9/rhel-guest-image:latest\n\n  # Needed for lvms-operator if you're using\n  - name: registry.redhat.io/openshift4/ose-must-gather:latest\n\n  # Heavily recommended to have a VDDK image when transferring from vSphere, best to create your own but this one generally works\n  - name: quay.io/jcall/vddk:latest\n\n  # Optional: KubeVirt Must gather support tools\n  - name: registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.17\n</code></pre>"},{"location":"examples/imageset-configs.html#other-handy-operators","title":"Other Handy Operators","text":"<p>NFD</p> <p>OADP</p> <p>MTC</p> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v1alpha2\nmirror:\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17\n    packages:\n    # Advanced Cluster Management for Kuberenetes\n    - name: advanced-cluster-management\n      channels:\n        - name: release-2.13\n    - name: multicluster-engine\n      channels:\n        - name: stable-2.8\n\n    # Advanced Cluster Security for Kuberenetes\n    - name: rhacs-operator\n      channels:\n      - name: stable\n\n    # OpenShift API for Data Protection (OADP) features provide options for backing up and restoring applications\n    - name: redhat-oadp-operator \n      channels:\n      - name: stable-1.4\n\n    # Migration Toolkit for containers to migrate workloads from one cluster to another\n    - name: mtc-operator\n      channels:\n      - name: release-v1.8\n\n    # Node feature discovery\n    - name: nfd\n      channels:\n      - name: stable\n\n    # Terminal that you can access from the WebUI\n    - name: web-terminal\n      channels:\n      - name: fast\n    - name: devworkspace-operator\n      channels:\n      - name: fast\n\n   # MetalLB Operator\n    - name: metallb-operator\n      channels:\n      - name: stable\n\n   # PTP Operator\n    - name: ptp-operator\n      channels:\n      - name: stable\n\n   # OpenShift Logging\n    - name: cluster-logging\n      channels:\n      - name: stable-6.2\n    - name: loki-operator\n      channels:\n      - name: stable-6.2\n\n   # OpenShift Service Mesh\n    - name: servicemeshoperator3\n      channels:\n      - name: stable\n    - name: kiali-ossm\n      channels:\n      - name: stable\n    - name: jaeger-product\n      channels:\n      - name: stable\n\n   # Cluster Observability Operator\n    - name: cluster-observability-operator\n      channels:\n      - name: stable\n\n   # Network Observability Operator\n    - name: netobserv-operator\n      channels:\n      - name: stable\n\n   # Keycloak Operator \n    - name: rhbk-operator\n      channels:\n      - name: stable-v26.0\n\n   # Cert Manager\n    - name: openshift-cert-manager-operator\n      channels:\n      - name: stable-v1\n\n  additionalImages:\n  - name: registry.redhat.io/rhel8/support-tools\n  - name: registry.redhat.io/rhel9/support-tools\n\n  # Universal Base Images\n  - name: registry.redhat.io/ubi9/ubi:latest\n  - name: registry.redhat.io/ubi8/ubi:latest\n\n  helm:\n    repositories:\n\n      # NFS CSI that can do dynamic provisioning off of NFS attached storage, or provide NFS storage from the cluster (community supported, need to add chart requested images to the additionalImages: dictionary)\n      - name: csi-driver-nfs\n        url: https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts\n        charts:\n          - name: csi-driver-nfs\n            version: 4.9.0\n</code></pre>"},{"location":"examples/imageset-configs.html#disa-stig-operators","title":"DISA STIG Operators","text":"<p>These aren't actually tied to the STIG, just makes it easier to actually apply it to your cluster</p> <pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.17\n    packages:\n    - name: compliance-operator\n      channels:\n      - name: stable\n    - name: file-integrity-operator\n      channels:\n      - name: stable\n</code></pre>"},{"location":"examples/imageset-configs.html#netapp-trident-operator","title":"NetApp (Trident Operator)","text":"<pre><code>---\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v2alpha1\n\nmirror:\n  operators:\n  # Certified Operators Catalog\n  - catalog: registry.redhat.io/redhat/certified-operator-index:v4.19\n    packages:\n\n    # Netapp Trident\n    - name: trident-operator\n      channels:\n        - name: stable\n</code></pre>"},{"location":"examples/install-configs.html","title":"install-config.yaml","text":""},{"location":"examples/install-configs.html#examples-of-install-configyaml-files","title":"Examples of install-config.yaml files","text":"<p>Red Hat Docs</p> <p>Red Hat Docs: Install Configuration Parameters</p> <p>Various examples of common install-configs. Sub in your data as appropriate.</p>"},{"location":"examples/install-configs.html#single-node-openshift-sno","title":"Single-Node OpenShift (SNO)","text":"install-config.yaml: SNO cluster<pre><code>apiVersion: v1\nbaseDomain: example.com # (1)! The base domain name of the cluster. All DNS records must be sub-domains of this base and include the cluster name.\ncompute:\n- architecture: amd64 # (2)! Specify the system architecture. Valid values are amd64, arm64, ppc64le, and s390x.\n  hyperthreading: Enabled\n  name: worker\n  replicas: 0 # (3)! This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  replicas: 1 # (4)! The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.\nmetadata:\n  name: cluster # (5)! The cluster name that you specified in your DNS records.\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14 # (6)! The network that the cluster shares for assigning IPs to PODS. Each node will get a /23 (500~ usable IP addresses). Make sure this IP space does not conflict with anything on your LAN.\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 172.16.1.0/24 # (7)! The network that connects the cluster to your LAN. This is the IP space that resides on your LAN.\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16 # (8)! Used for internal service objects. Make sure this IP space does not conflict with anything on your LAN.\nplatform:\n  none: {} # (9)! You must set the platform to none for a single-node cluster. You can set the platform to vsphere, baremetal, or none for multi-node clusters.\nfips: false # (10)! Boolean: Either true or false to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that OpenShift Container Platform runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead\npullSecret: '{\"auths\":{\"registry.example.com:8443\": {\"auth\": \"am9zaDpLSW....\",\"email\": \"\"}}}' # (11)! A pull secret for your internal image registry\nsshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABg....' # (12)! Public ssh key that you define. This key will give ssh access to the nodes through the 'core' user. This is the only way to ssh into the nodes\nadditionalTrustBundle: | # (13)! The rootCA.pem certificate of your internal image registry\n  -----BEGIN CERTIFICATE-----\n  MIID1jCCAr6gAwIBAgIUZ11j30+eBRjNEl7IPufQdzMl6oAwDQYJKoZIhvcNAQEL\n  BQAwajELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\n  azENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xGTAXBgNVBAMMEHJl\n  ...\n  -----END CERTIFICATE----- \nimageDigestSources: # (14)! Your image mirrors, this in the idms-oc-mirror.yaml file generated from oc mirror. i.e. /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml \n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n</code></pre> <ol> <li>The base domain name of the cluster. All DNS records must be sub-domains of this base and include the cluster name.</li> <li>Specify the system architecture. Valid values are <code>amd64</code>, <code>arm64</code>, <code>ppc64le</code>, and <code>s390x</code>.</li> <li>This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.</li> <li>The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.</li> <li>The cluster name that you specified in your DNS records.</li> <li>The network that the cluster shares for assigning IPs to PODS. Each node will get a /23 (500~ usable IP addresses). Make sure this IP space does not conflict with anything on your LAN.</li> <li>The network that connects the cluster to your LAN. This is the IP space that resides on your LAN.</li> <li>Used for internal service objects. Make sure this IP space does not conflict with anything on your LAN.</li> <li>You must set the platform to <code>none</code> for a single-node cluster. You can set the platform to <code>vsphere</code>, <code>baremetal</code>, or <code>none</code> for multi-node clusters.</li> <li>Boolean: Either true or false to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that OpenShift Container Platform runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead</li> <li>A pull secret for your internal image registry</li> <li>Public ssh key that you define. This key will give ssh access to the nodes through the 'core' user. This is the only way to ssh into the nodes</li> <li>The rootCA.pem certificate of your internal image registry</li> <li>Your image mirrors, this in the idms-oc-mirror.yaml file generated from oc mirror. i.e. /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml </li> </ol>"},{"location":"examples/install-configs.html#ha-openshift-3-master-nodes-and-2-worker-nodes","title":"HA OpenShift - 3 master nodes and 2 worker nodes","text":"install-config.yaml: HA cluster<pre><code>apiVersion: v1\nbaseDomain: example.com # (1)! The base domain name of the cluster. All DNS records must be sub-domains of this base and include the cluster name.\ncompute:\n- architecture: amd64 # (2)! Specify the system architecture. Valid values are amd64, arm64, ppc64le, and s390x.\n  hyperthreading: Enabled\n  name: worker\n  replicas: 2 # (3)! This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  replicas: 3 # (4)! The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.\nmetadata:\n  name: cluster # (5)! The cluster name that you specified in your DNS records.\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14 # (6)! The network that the cluster shares for assigning IPs to PODS. Each node will get a /23 (500~ usable IP addresses). Make sure this IP space does not conflict with anything on your LAN.\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 172.16.1.0/24 # (7)! The network that connects the cluster to your LAN. This is the IP space that resides on your LAN.\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16 # (8)! Used for internal service objects. Make sure this IP space does not conflict with anything on your LAN.\nplatform:\n  baremetal:\n    apiVIPs:\n    - 172.16.1.5 # (9)! API ip address\n    ingressVIPs:\n    - 172.16.1.6 # (10)! Ingress API ip address, the *.apps A record\nfips: false # (11)! Boolean: Either true or false to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that OpenShift Container Platform runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead\npullSecret: '{\"auths\":{\"registry.example.com:8443\": {\"auth\": \"am9zaDpLSW....\",\"email\": \"\"}}}' # (12)! A pull secret for your internal image registry\nsshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABg....' # (13)! Public ssh key that you define. This key will give ssh access to the nodes through the 'core' user. This is the only way to ssh into the nodes\nadditionalTrustBundle: | # (14)! The rootCA.pem certificate of your internal image registry\n  -----BEGIN CERTIFICATE-----\n  MIID1jCCAr6gAwIBAgIUZ11j30+eBRjNEl7IPufQdzMl6oAwDQYJKoZIhvcNAQEL\n  BQAwajELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\n  azENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xGTAXBgNVBAMMEHJl\n  ...\n  -----END CERTIFICATE----- \nimageDigestSources: # (15)! Your image mirrors, this in the idms-oc-mirror.yaml file generated from oc mirror. i.e. /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml \n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n</code></pre> <ol> <li>The base domain name of the cluster. All DNS records must be sub-domains of this base and include the cluster name.</li> <li>Specify the system architecture. Valid values are <code>amd64</code>, <code>arm64</code>, <code>ppc64le</code>, and <code>s390x</code>.</li> <li>This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.</li> <li>The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.</li> <li>The cluster name that you specified in your DNS records.</li> <li>The network that the cluster shares for assigning IPs to PODS. Each node will get a /23 (500~ usable IP addresses). Make sure this IP space does not conflict with anything on your LAN.</li> <li>The network that connects the cluster to your LAN. This is the IP space that resides on your LAN.</li> <li>Used for internal service objects. Make sure this IP space does not conflict with anything on your LAN.</li> <li>API ip address</li> <li>Ingress API ip address, the *.apps A record</li> <li>Boolean: Either true or false to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that OpenShift Container Platform runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead</li> <li>A pull secret for your internal image registry</li> <li>Public ssh key that you define. This key will give ssh access to the nodes through the 'core' user. This is the only way to ssh into the nodes</li> <li>The rootCA.pem certificate of your internal image registry</li> <li>Your image mirrors, this in the idms-oc-mirror.yaml file generated from oc mirror. i.e. /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml</li> </ol>"},{"location":"examples/install-configs.html#cluster-capabilites","title":"Cluster Capabilites","text":"<p>Capabilites config for the v4.18 install</p> <p>By default, all capabilites will be included in the cluster for the version you are installing. You can choose to remove capabilites at install time. You can enable cluster capabilities at anytime after installation</p> <p>Note</p> <p>You cannot cannot disable a cluster capability after it is enabled.</p> <p>If you customize your cluster by enabling or disabling specific cluster capabilities, you must manually maintain your <code>install-config.yaml</code> file. New OpenShift Container Platform updates might declare new capability handles for existing components, or introduce new components altogether. Users who customize their <code>install-config.yaml</code> file should consider periodically updating their <code>install-config.yaml</code> file as OpenShift Container Platform is updated.</p> install-config.yaml: SNO cluster with capabilites defined<pre><code>apiVersion: v1\nbaseDomain: example.com\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  replicas: 0\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  replicas: 1\nmetadata:\n  name: cluster\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 172.16.1.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  none: {}\ncapabilities:\n  baselineCapabilitySet: None # (1)! Easiest method is to set the baseline to None, then choose enabled capabilites in the additionalEnabledCapabilities stanza\n  additionalEnabledCapabilities: # (2)! Here define what capabilites you want the cluster to have. Remove whatever you don't want\n    - Build\n    - CSISnapshot\n    - CloudControllerManager\n    - CloudCredential\n    - Console\n    - DeploymentConfig\n    - ImageRegistry\n    - Ingress                    # OpenShift v4.16+\n    - Insights                   # If you don't want to unclude Insights since the cluster is disconnected, although disconnected Insights is coming soon..\n    - MachineAPI                 # Mandatory with \"baremetal\" IPI\n    - NodeTuning\n    - OperatorLifecycleManager\n    - OperatorLifecycleManagerV1 # OpenShift v4.18+\n    - Storage\n    - baremetal\n    - marketplace\n    - openshift-samples\nfips: false\npullSecret: '{\"auths\":{\"registry.example.com:8443\": {\"auth\": \"am9zaDpLSW....\",\"email\": \"\"}}}'\nsshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABg....'\nadditionalTrustBundle: |\n  -----BEGIN CERTIFICATE-----\n  MIID1jCCAr6gAwIBAgIUZ11j30+eBRjNEl7IPufQdzMl6oAwDQYJKoZIhvcNAQEL\n  BQAwajELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\n  azENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xGTAXBgNVBAMMEHJl\n  ...\n  -----END CERTIFICATE----- \nimageDigestSources:\n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n</code></pre> <ol> <li>Easiest method is to set the baseline to None, then choose enabled capabilites in the additionalEnabledCapabilities stanza</li> <li>Here define what capabilites you want the cluster to have. Remove whatever you don't want</li> </ol>"},{"location":"install/index.html","title":"Disconnected Cluster Install","text":"<p>Red Hat Docs</p> <p>Now that the images have been uploaded/mirrored to your environment we can build the cluster ISO and boot OpenShift.</p> <p>The general flow of a disconnected OpenShift install: </p> <ol> <li>Create the required cluster config files.</li> <li>Create the ISO defined by the cluster config files.</li> <li>Boot the ISO on your machines.</li> <li>Configure your cluster to use the resources generated by the oc-mirror plugin.</li> <li>Profit??</li> </ol>"},{"location":"install/config.html","title":"3.1 Cluster Configs and ISO building","text":""},{"location":"install/config.html#creating-cluster-configs-and-building-the-agent-iso","title":"Creating cluster configs and building the agent ISO","text":"<p>Red Hat Docs: Install Configuration Parameters</p> <p>Here is where the cluster is defined. We'll use two files, <code>install-config.yaml</code> and <code>agent-config.yaml</code> along with the <code>openshift-install</code> binary we extracted earlier to create the install <code>agent.iso</code>. Examples are given below. Create a directory somewhere to house the config files. This directory will also hold the credentials for API and console access to the cluster when it is built.</p> <ol> <li>Install the <code>nmstate</code> package <pre><code>sudo dnf install /usr/bin/nmstatectl -y\n</code></pre></li> <li> <p>Create a directory to store the cluster configuration, we'll call this directory <code>my_cluster</code> for this example and it's in the users $HOME directory <pre><code>mkdir ~/my_cluster\n</code></pre></p> </li> <li> <p>Get the rootCA of your Registry. This is required for the <code>install-config.yaml</code> file.</p> <ol> <li> <p>Grab the cert using openssl or by some other means <pre><code>openssl s_client -connect registry.example.com:8443 -showcerts | awk '/BEGIN/,/END/{print $0}' | tee ./rootCA.pem\ncat rootCA.pem\n\n# If you want your system to trust the registry CA\nsudo cp ./rootCA.pem /etc/pki/ca-trust/source/anchors/\nsudo update-ca-trust\n</code></pre></p> </li> <li> <p>If you installed Red Hat Quay following this documentation, it is also located on disk where you specified the <code>quayRoot</code>. <pre><code>cat /opt/quay-root/quay-rootCA/rootCA.pem\n\n# If you want your system to trust the registry CA\nsudo cp /opt/quay-root/quay-rootCA/rootCA.pem /etc/pki/ca-trust/source/anchors/\nsudo update-ca-trust\n</code></pre></p> </li> </ol> <p>Info</p> <p>For the <code>additionalTrustBundle</code>, the data must be indented as displayed in the example <code>install-config.yaml</code> below. You can add indentation with a quick <code>sed</code> command to then paste it into your install-config.yaml. <pre><code>sed \"s/^/  /\" rootCA.pem\n</code></pre></p> </li> <li> <p>Get the pull secret for your mirror registry. This is required for the <code>install-config.yaml</code> file. Using <code>jq</code> you can print it to stdout in a single line like so: <pre><code>jq -c . $XDG_RUNTIME_DIR/containers/auth.json\n</code></pre></p> </li> <li> <p>Create the files <code>install-config.yaml</code> and <code>agent-config.yaml</code> in the directory you defined.</p> </li> </ol> <p>The example below builds a bare metal compact cluster (3 master/control-plane/worker nodes) with static IP's and no external load-balancer. The cluster is called <code>cluster.example.com</code>. Each node is called m(1-3).cluster.example.com. You can use these procedures as a basis and modify according to your requirements.</p> <ul> <li>install-config.yaml: This defines your cluster configuration, click on the <code>+</code> signs to get a general description of some the values</li> </ul> <p>You can look at several install-config.yaml examples here in this document</p> install-config.yaml: Compact cluster<pre><code>apiVersion: v1\nbaseDomain: example.com # (1)! The base domain name of the cluster. All DNS records must be sub-domains of this base and include the cluster name.\ncompute:\n- architecture: amd64 # (2)! Specify the system architecture. Valid values are amd64, arm64, ppc64le, and s390x.\n  hyperthreading: Enabled\n  name: worker\n  replicas: 0 # (3)! This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  replicas: 3 # (4)! The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.\nmetadata:\n  name: cluster # (5)! The cluster name that you specified in your DNS records.\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14 # (6)! The network that the cluster shares for assigning IPs to PODS. Each node will get a /23 (500~ usable IP addresses). Make sure this IP space does not conflict with anything on your LAN.\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 172.16.1.0/24 # (7)! The network that connects the cluster to your LAN. This is the IP space that resides on your LAN.\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16 # (8)! Used for internal service objects. Make sure this IP space does not conflict with anything on your LAN.\nplatform:\n  baremetal:\n    apiVIPs:\n    - 172.16.1.5 # (9)! API ip address\n    ingressVIPs:\n    - 172.16.1.6 # (10)! Ingress API ip address, the *.apps A record\n    provisioningNetwork: Disabled\nfips: false # (11)! Boolean: Either true or false to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that OpenShift Container Platform runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead\npullSecret: '{\"auths\":{\"registry.example.com:8443\": {\"auth\": \"am9zaDpLSW....\",\"email\": \"\"}}}' # (12)! A pull secret for your internal image registry. Best practive is for this secret to only have pull permissions\nsshKey: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABg....' # (13)! Public ssh key that you define. This key will give ssh access to the nodes through the 'core' user. This is the only way to ssh into the nodes by default\nadditionalTrustBundle: | # (14)! The rootCA.pem certificate of your internal image registry. Note the indentation\n  -----BEGIN CERTIFICATE-----\n  MIID1jCCAr6gAwIBAgIUZ11j30+eBRjNEl7IPufQdzMl6oAwDQYJKoZIhvcNAQEL\n  BQAwajELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\n  azENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xGTAXBgNVBAMMEHJl\n  ...\n  -----END CERTIFICATE----- \nadditionalTrustBundlePolicy: Always\nimageDigestSources: # (15)! Your image mirrors, this in the idms-oc-mirror.yaml file generated from oc mirror. i.e. /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml \n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release-images # (16)! # You must have a direct reference to both the openshift-release-dev/ocp-release and openshift-release-dev/ocp-v4.0-art-dev paths. These two are the only ones required to complete an installation of OpenShift\n  source: quay.io/openshift-release-dev/ocp-release\n- mirrors:\n  - registry.example.com:8443/ocp/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n</code></pre> <ol> <li>The base domain name of the cluster. All DNS records must be sub-domains of this base and include the cluster name.</li> <li>Specify the system architecture. Valid values are <code>amd64</code>, <code>arm64</code>, <code>ppc64le</code>, and <code>s390x</code>.</li> <li>This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.</li> <li>The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.</li> <li>The cluster name that you specified in your DNS records.</li> <li>The network that the cluster shares for assigning IPs to PODS. Each node will get a /23 (500~ usable IP addresses). Make sure this IP space does not conflict with anything on your LAN.</li> <li>The network that connects the cluster to your LAN. This is the IP space that resides on your LAN.</li> <li>Used for internal service objects. Make sure this IP space does not conflict with anything on your LAN.</li> <li>API ip address</li> <li>Ingress API ip address, the *.apps A record</li> <li>Boolean: Either true or false to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the Red Hat Enterprise Linux CoreOS (RHCOS) machines that OpenShift Container Platform runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with RHCOS instead</li> <li>A pull secret for your internal image registry. Best practive is for this secret to only have pull permissions</li> <li>Public ssh key that you define. This key will give ssh access to the nodes through the 'core' user. This is the only way to ssh into the nodes by default</li> <li>The rootCA.pem certificate of your internal image registry. Note the indentation</li> <li>Your image mirrors, this in the idms-oc-mirror.yaml file generated from oc mirror. i.e. /opt/4.17-mirrordata/working-dir/cluster-resources/idms-oc-mirror.yaml </li> <li>You must have a direct reference to both the openshift-release-dev/ocp-release and openshift-release-dev/ocp-v4.0-art-dev paths. These two are the only ones required to complete an installation of OpenShift</li> </ol> <ul> <li>agent-config.yaml: This defines your node(s) configuration, click on the <code>+</code> signs to get a general description of some of the values</li> </ul> <p>You can look at several agent-config.yaml examples here in this document</p> agent-config.yaml: Static IP assignment<pre><code>apiVersion: v1alpha1\nkind: AgentConfig\nmetadata:\n  name: cluster # (1)! The cluster name that you specified in your DNS records.\nrendezvousIP: 172.16.1.10 # (2)! Can be the IP of any one of the master nodes. This node will become bootstrap machine during install. A worker cannot be the rendezvous machine.\nadditionalNTPSources: # (3)! Optional but recommended for disconnected installs. The install can work without NTP but it'll rely on the system clocks to be correct which may not always be true. IP address or hostname is accepatable \n- ntp.example.com \nhosts:\n- hostname: m1.cluster.example.com # (4)! Hostname of the node, must be resolvable by dns.\n  role: master # (5)! Recommended to explicitly define roles for your hosts, especially if you're defining masters and workers as they would otherwise be applied at random.\n  interfaces:\n  - name: enp6s18 # (6)! Name of the interface. If you do not know it, the installer scripts will detect the actual name by mac-address.\n    macAddress: BC:24:11:EE:DD:C1 # (7)! Required! The MAC address of an interface on the host, used to determine which host to apply the configuration to.\n  networkConfig:\n    interfaces:\n    - name: enp6s18\n      type: ethernet\n      state: up\n      mac-address: BC:24:11:EE:DD:C1\n      ipv4:\n        enabled: true\n        address:\n        - ip: 172.16.1.10 # (8)! The static IP address of the target bare metal host.\n          prefix-length: 24 # (9)! The static IP address\u2019s subnet prefix for the target bare metal host.\n        dhcp: false\n    dns-resolver:\n      config:\n        server:\n        - 172.16.1.254 # (10)! The DNS server for the target bare metal host.\n    routes:\n      config:\n      - destination: 0.0.0.0/0\n        next-hop-address: 172.16.1.254 # (11)! The default gateway, or default route of your node. This must be in the same subnet as the IP address set for the specified interface.\n        next-hop-interface: enp6s18\n        table-id: 254\n\n- hostname: m2.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C2\n  networkConfig:\n    interfaces:\n    - name: enp6s18\n      type: ethernet\n      state: up\n      mac-address: BC:24:11:EE:DD:C2\n      ipv4:\n        enabled: true\n        address:\n        - ip: 172.16.1.11\n          prefix-length: 24\n        dhcp: false\n    dns-resolver:\n      config:\n        server:\n        - 172.16.1.254\n    routes:\n      config:\n      - destination: 0.0.0.0/0\n        next-hop-address: 172.16.1.254\n        next-hop-interface: enp6s18\n        table-id: 254\n\n- hostname: m3.cluster.example.com\n  role: master\n  interfaces:\n  - name: enp6s18\n    macAddress: BC:24:11:EE:DD:C3\n  networkConfig:\n    interfaces:\n    - name: enp6s18\n      type: ethernet\n      state: up\n      mac-address: BC:24:11:EE:DD:C3\n      ipv4:\n        enabled: true\n        address:\n        - ip: 172.16.1.12\n          prefix-length: 24\n        dhcp: false\n    dns-resolver:\n      config:\n        server:\n        - 172.16.1.254\n    routes:\n      config:\n      - destination: 0.0.0.0/0\n        next-hop-address: 172.16.1.254\n        next-hop-interface: enp6s18\n        table-id: 254\n</code></pre> <ol> <li>The cluster name that you specified in your DNS records.</li> <li>Can be the IP of any one of the master nodes. This node will become bootstrap machine during install. A worker cannot be the rendezvous machine.</li> <li>Optional but recommended for disconnected installs. The install can work without NTP but it'll rely on the system clocks to be correct which may not always be true. IP address or hostname is accepatable </li> <li>Hostname of the node, must be resolvable by DNS.</li> <li>Recommended to explicitly define roles for your hosts, especially if you're defining masters and workers as they would otherwise be applied at random.</li> <li>Name of the interface. If you do not know it, the installer scripts will detect the actual name by mac-address.</li> <li>Required! The MAC address of an interface on the host, used to determine which host to apply the configuration to.</li> <li>The static IP address of the target bare metal host.</li> <li>The static IP address\u2019s subnet prefix for the target bare metal host.</li> <li>The DNS server for the target bare metal host.</li> <li>The default gateway, or default route of your node. This must be in the same subnet as the IP address set for the specified interface.</li> </ol>"},{"location":"install/config.html#validation-checks","title":"Validation checks","text":"<p>The Agent-based Installer performs validation checks on user defined YAML files before the ISO is created. Once the validations are successful, the agent ISO is created.</p> <p><code>install-config.yaml</code></p> <ul> <li><code>baremetal</code>, <code>vsphere</code> and <code>none</code> platforms are supported.</li> <li>If <code>none</code> is used as a platform, the number of control plane replicas must be <code>1</code> and the total number of worker replicas must be <code>0</code>.</li> <li>The <code>networkType</code> parameter must be <code>OVNKubernetes</code> in the case of none platform. Generally stick with <code>OVNKubernetes</code> for all installs.</li> <li><code>apiVIPs</code> and <code>ingressVIPs</code> parameters must be set for bare metal and vSphere platforms.</li> <li>Some host-specific fields in the bare metal platform configuration that have equivalents in <code>agent-config.yaml</code> file are ignored. A warning message is logged if these fields are set.</li> </ul> <p><code>agent-config.yaml</code></p> <ul> <li>Each interface must have a defined MAC address. Additionally, all interfaces must have a different MAC address.</li> <li>At least one interface must be defined for each host.</li> <li>World Wide Name (WWN) vendor extensions are not supported in root device hints.</li> <li>The <code>role</code> parameter in the <code>host</code> object must have a value of either <code>master</code> or <code>worker</code>.</li> </ul>"},{"location":"install/config.html#creating-the-agent-image","title":"Creating the agent image","text":"<ol> <li> <p>Your cluster build directory should look like this <pre><code>tree my_cluster\n</code></pre> Example Output<pre><code>my_cluster\n\u251c\u2500\u2500 agent-config.yaml\n\u2514\u2500\u2500 install-config.yaml\n\n0 directories, 2 files\n</code></pre></p> </li> <li> <p>Make a copy of this directory and it's contents. When you run <code>openshift-install</code> against it, all the files are consumed to build the image. <pre><code>cp -r my_cluster/ my_cluster_backup/\n</code></pre></p> </li> <li> <p>Create the agent image. It will be saved in the target directory that had the configs. <pre><code>openshift-install --dir my_cluster/ agent create image\n</code></pre> Example Output<pre><code>INFO Configuration has 3 master replicas and 0 worker replicas\nWARNING The imageDigestSources configuration in install-config.yaml should have at least one source field matching the releaseImage value registry.example.com:8443/ocp/openshift/release-images@sha256:fd8f5562f0403504b35cc62e064b04c34e6baeb48384bddebffc98c3c69a2af3\nINFO The rendezvous host IP (node0 IP) is 172.16.1.10\nINFO Extracting base ISO from release payload\nINFO Base ISO obtained from release and cached at [/home/admin/.cache/agent/image_cache/coreos-x86_64.iso]\nINFO Consuming Install Config from target directory\nINFO Consuming Agent Config from target directory\nINFO Generated ISO at my_cluster/agent.x86_64.iso.\n</code></pre></p> <ul> <li>For FIPS, do the same thing but use the FIPS binary <pre><code>openshift-install-fips --dir my_cluster/ agent create image\n</code></pre></li> </ul> <p>Info</p> <p>If you have built an ISO before, there may be issues with the caching system that will cause a new build to fail.</p> <p>Remove the existing cache, it will exist in the <code>$HOME</code> of the installation user</p> <pre><code>rm -rf ~/.cache/agent\n</code></pre> </li> </ol>"},{"location":"install/install.html","title":"3.2 Installing the cluster","text":""},{"location":"install/install.html#booting-the-agent-iso-and-watching-the-installation","title":"Booting the agent ISO and watching the installation","text":"<ol> <li> <p>Boot the agent.iso on your hardware. This example output is for a single node OpenShift install, the node is named <code>sno.cluster.example.com</code>. The commands are the same for whatever type of cluster your are installing with the agent based installer. Normally installs take around 45 minutes give or take.</p> <p>Info</p> <p>When you boot your ISO, make sure to set the ISO as a one-time boot option. The node(s) will reboot automatically during install and you don't want them rebooting into the installation ISO accidentally.</p> <p>For FIPS, do the same thing but use the FIPS variant of the openshift-install binary</p> <pre><code>openshift-install-fips --dir my_cluster/ agent wait-for &lt;command&gt;\n</code></pre> </li> <li> <p>Watch for the bootstrap to complete and the Kube API initialization. These commands should tell you if there's anything wrong during the bootstrapping process.     <pre><code>openshift-install --dir my_cluster/ agent wait-for bootstrap-complete --log-level=info\n</code></pre> Example Output<pre><code>INFO Waiting for cluster install to initialize. Sleeping for 30 seconds\nINFO Cluster is not ready for install. Check validations\nINFO Registered infra env\nINFO Host b6076a53-c453-4046-8d95-2dd2cfe96e25: Successfully registered\nWARNING Cluster validation: The cluster has hosts that are not ready to install.\nINFO Host 5ab09259-3ac4-4259-98b0-b5ddc033e701: Successfully registered\nWARNING Host sno.cluster.example.com validation: Host couldn't synchronize with any NTP server\nINFO Host sno.cluster.example.com validation: Host NTP is synced\nINFO Host sno.cluster.example.com: validation 'ntp-synced' is now fixed\nINFO Host sno.cluster.example.com: updated status from insufficient to known (Host is ready to be installed)\nINFO Cluster is ready for install\nINFO Cluster validation: All hosts in the cluster are ready to install.\nINFO Preparing cluster for installation\nINFO Host sno.cluster.example.com: updated status from known to preparing-for-installation (Host finished successfully to prepare for installation)\nINFO Host sno.cluster.example.com: updated status from preparing-for-installation to preparing-successful (Host finished successfully to prepare for installation)\nINFO Cluster installation in progress\nINFO Host sno.cluster.example.com: updated status from preparing-successful to installing (Installation is in progress)\nINFO Host: sno.cluster.example.com, reached installation stage Installing: bootstrap\nINFO Host: sno.cluster.example.com, reached installation stage Waiting for bootkube\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 6%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 12%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 19%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 25%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 35%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 40%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 45%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 51%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 58%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 64%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 70%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 76%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 82%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 88%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 93%\nINFO Host: sno.cluster.example.com, reached installation stage Writing image to disk: 100%\nINFO Bootstrap Kube API Initialized\nINFO Bootstrap configMap status is complete\nINFO Bootstrap is complete\nINFO cluster bootstrap is complete\n</code></pre></p> </li> <li> <p>Once the bootstrap fully completes, the command will exit and dump you to back to the terminal. Now you can switch to waiting for the install to complete.     <pre><code>openshift-install --dir my_cluster/ agent wait-for install-complete\n</code></pre> Example Output<pre><code>INFO Bootstrap Kube API Initialized\nINFO Bootstrap configMap status is complete\nINFO Bootstrap is complete\nINFO cluster bootstrap is complete\nINFO Cluster is installed\nINFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run\nINFO     export KUBECONFIG=/home/admin/my_cluster/auth/kubeconfig\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.cluster.example.com\nINFO Login to the console with user: \"kubeadmin\", and password: \"gbEsF-FxsIQ-Y7zNt-P5xvv\"\n</code></pre></p> </li> <li> <p>Now that the cluster is installed, use the information provided from the end of the log to access the API/WebGUI.</p> </li> <li>The API will be available immediately, but it may take a few more minutes before the console WebUI is accessable. The ClusterOperators generally take a bit longer to fully initialize.<ul> <li>Check the cluster operators status using the oc command line tool:  <pre><code>oc get clusteroperators\n</code></pre></li> </ul> </li> <li>Wait at least 24 hours before rebooting the cluster or making any changes that will reboot the nodes. KubeAPI certificates will be propagated to all components in the cluster during the first 24 hours. If the nodes get rebooted before the certs get approved by all components, there is a high chance the cluster will not initilize correctly and be unhappy. You'll have to manually approve them on all the nodes through an SSH connection. </li> </ol>"},{"location":"install/install.html#troubleshooting","title":"Troubleshooting","text":"<p>Things may not go as planned so sometimes manual intervention may be required. The <code>openshift-install --dir my_cluster/ agent wait-for</code> commands should let you know if an error occurs, but sometimes it's best to get directly on the nodes and troubleshoot.</p>"},{"location":"install/install.html#direct-ssh-access","title":"Direct SSH access","text":"<ul> <li>Since there's an SSH key injected into the ISO, you can SSH directly to the node using the same ssh key specified in the install-config.yaml. <pre><code>ssh core@sno.cluster.example.com\n</code></pre> Example Commands<pre><code>...\n# Become root\n[core@sno ~]$ sudo -i\n[root@sno ~]$\n\n# From here we can poke around the system to view any potential issues\n[root@sno ~]$ journalctl -f --no-pager\n</code></pre></li> </ul>"},{"location":"install/install.html#kubeapi-certificate-issues","title":"KubeAPI Certificate Issues","text":"<p>RH Docs</p> <p>If you powered down the cluster before 24 hours, or the cluster has been down for a extended amount of time (&lt; 30 days), there may be some kube certificate issues that will cause the KubeAPI fail to initialize. The node-bootstrapper CSRs has likely need to be manually approved. </p> <ul> <li> <p>SSH to the master node(s), then use the <code>localhost.kubeconfig</code> to inspect the Certificate Signing Requests (csr's)  <pre><code>oc get csr --kubeconfig /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig\n</code></pre> Example Output<pre><code>NAME        AGE     SIGNERNAME                                    REQUESTOR                                                                   REQUESTEDDURATION   CONDITION\ncsr-8dthq   91m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-92ngr   60m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-dkm6t   168m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-fqq68   13m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-g4mgs   137m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-g6hjc   106m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-h9hjd   3h34m   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-hssjx   3h3m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-j2wbc   152m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-k7g7l   75m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-kgqnn   4h5m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-ln289   29m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-mx4pn   3h19m   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-n9jf7   122m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-nll8c   3h50m   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-qvt5r   44m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\n</code></pre></p> </li> <li> <p>Since they are all pending, but there's no valid server CA to approve them, grab the names of the csr's and pipe it to <code>xargs</code> to approve all of them using the localhost.kubeconfig <pre><code>oc get csr -o name --kubeconfig /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig | xargs oc adm certificate approve --kubeconfig /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig\n</code></pre></p> </li> <li> <p>Wait a bit and you should see the cluster reconcile and become available</p> </li> </ul>"},{"location":"optional/corepasswd.html","title":"Core User Password","text":""},{"location":"optional/corepasswd.html#setting-a-password-for-the-core-user","title":"Setting a password for the <code>core</code> user","text":"<p>By default, Red Hat Enterprise Linux CoreOS (RHCOS) creates a user named <code>core</code> on the nodes in your cluster. You can use the <code>core</code> user to access the node through a cloud provider serial console or a bare metal baseboard controller manager (BMC). </p> <p>This can be helpful, for example, if a node is down and you cannot access that node by using SSH or the <code>oc debug node</code> command. However, by default, there is no password for this user, so you cannot log in without creating one. You can create a password for the <code>core</code> user by using a machine config.</p>"},{"location":"optional/corepasswd.html#openshift-47-to-412-procedure","title":"OpenShift 4.7 to 4.12 Procedure","text":"<p>Red Hat KCS Article</p> <ol> <li> <p>Create a base64-encoded string in the format <code>username:password</code>, with the username as <code>core</code> and the password being hashed with SHA512 (<code>openssl passwd -6</code>) in order to avoid storing cleartext passwords. Replace <code>MYPASSWORD</code> in the command below with the password of your choice:</p> <pre><code>MYBASE64STRING=$(echo core:$(printf \"MYPASSWORD\" | openssl passwd -6 --stdin) | base64 -w0)\n</code></pre> </li> <li> <p>Using the template below as an example, create a <code>MachineConfig</code> object that accomplishes two tasks:</p> <ol> <li> <p>Writes the base64-encoded string generated above on the desired nodes' filesystem as the file <code>/etc/core.passwd</code></p> </li> <li> <p>Sets up a new systemd unit on the desired nodes to run the <code>chpasswd</code> command during the boot process using the file written above as input (The <code>-e</code> flag is used to tell <code>chpasswd</code> to expect an encrypted/hashed password).</p> </li> </ol> <p><pre><code>cat &lt;&lt; EOF &gt; 99-set-core-passwd.yaml\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: worker\n  name: 99-worker-set-core-passwd\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    storage:\n      files:\n      - contents:\n          source: data:text/plain;charset=utf-8;base64,$MYBASE64STRING\n        mode: 420\n        overwrite: true\n        path: /etc/core.passwd\n    systemd:\n      units:\n      - name: set-core-passwd.service\n        enabled: true\n        contents: |\n          [Unit]\n          Description=Set 'core' user password for out-of-band login\n          [Service]\n          Type=oneshot\n          ExecStart=/bin/sh -c 'chpasswd -e &lt; /etc/core.passwd'\n          [Install]\n          WantedBy=multi-user.target\nEOF\n</code></pre> <pre><code>oc create -f 99-set-core-passwd.yaml\n</code></pre></p> </li> <li> <p>As the <code>MachineConfig</code> is applied, the file containing the hashed password will be created and a new systemd unit will be configured to run the <code>chpasswd</code> command on the nodes' next boot process, setting a password for the <code>core</code> user and thus allowing terminal login via virtual console.</p> <p>Note</p> <p>Be aware that SSH password-based login would not be possible still as it is disabled by default on RHCOS <code>sshd</code> configuration, allowing only key-based authentication. Also, these steps could be taken before the issue arises, as a safeguard.</p> </li> </ol>"},{"location":"optional/corepasswd.html#openshift-413-and-up-procedure","title":"OpenShift 4.13 and up Procedure","text":"<p>Red Hat Docs</p> <p>You can create a password for the <code>core</code> user by using a machine config. The Machine Config Operator (MCO) assigns the password and injects the password into the <code>/etc/shadow</code> file, allowing you to log in with the <code>core</code> user. The MCO does not examine the password hash. As such, the MCO cannot report if there is a problem with the password.</p> <p>Note</p> <p>The password works only through a cloud provider serial console or a BMC. It does not work with SSH.</p> <p>If you have a machine config that includes an <code>/etc/shadow</code> file or a systemd unit that sets a password, it takes precedence over the password hash.</p> <p>You can change the password, if needed, by editing the machine config you used to create the password. Also, you can remove the password by deleting the machine config. Deleting the machine config does not remove the user account.</p> <ol> <li> <p>Using a tool that is supported by your operating system, create a hashed password. For example, create a hashed password using <code>mkpasswd</code> by running the following command:</p> <p><pre><code>mkpasswd -m SHA-512 testpass\n</code></pre> Example Output<pre><code>$6$CBZwA6s6AVFOtiZe$aUKDWpthhJEyR3nnhM02NM1sKCpHn9XN.NPrJNQ3HYewioaorpwL3mKGLxvW0AOb4pJxqoqP4nFX77y0p00.8.\n</code></pre> 2. Create a machine config file that contains the core username and the hashed password:</p> <pre><code>apiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfig\nmetadata:\n  labels:\n    machineconfiguration.openshift.io/role: worker\n  name: set-core-user-password\nspec:\n  config:\n    ignition:\n      version: 3.2.0\n    passwd:\n      users:\n      - name: core\n        passwordHash: &lt;password&gt;\n</code></pre> <ul> <li>The user name must be <code>core</code>.</li> <li><code>&lt;password&gt;</code> is the hashed password to use with the core account.</li> </ul> </li> <li> <p>Create the machine config by running the following command     <pre><code>oc create -f &lt;file-name&gt;.yaml\n</code></pre></p> <p>The nodes do not reboot and should become available in a few moments. You can use the <code>oc get mcp</code> to watch for the machine config pools to be updated Example Output<pre><code>NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nmaster   rendered-master-d686a3ffc8fdec47280afec446fce8dd   True      False      False      3              3                   3                     0                      64m\nworker   rendered-worker-4605605a5b1f9de1d061e9d350f251e5   False     True       False      3              0                   0                     0                      64m\n</code></pre></p> </li> </ol>"},{"location":"optional/registry.html","title":"Red Hat Mirror Registry","text":""},{"location":"optional/registry.html#mirror-registry-for-red-hat-openshift","title":"Mirror Registry for Red Hat OpenShift","text":"<p>Red Hat Docs</p> <ul> <li>This is a quick install config for the Red Hat provided mirror registry if you want to use it. Make sure you have transferred the <code>mirror-registry-amd64.tar.gz</code> from the connected side.<ul> <li>There are a few ways to install this, refer to the Red Hat docs above for additional installation examples and configurations.</li> <li>This example will go over installing the mirror registry to a local host called <code>registry.example.com</code>, and storing the persistent data in a directory on disk as a non-root user in <code>/opt</code></li> </ul> </li> </ul> <ol> <li> <p>Prerequisites</p> <ul> <li>Red Hat Enterprise Linux (RHEL) 8 and 9 with Podman 3.4.2 or later and OpenSSL installed.</li> <li>Fully qualified domain name for the Red Hat Quay service, which must resolve through a DNS server. (i.e. <code>registry.example.com</code>)</li> <li>Key-based SSH connectivity on the target host. SSH keys are automatically generated for local installs. For remote hosts, you must generate your own SSH keys.</li> <li>2 or more vCPUs.</li> <li>8 GB or more of RAM.</li> <li>About 30 GB for OpenShift Container Platform release images, or about 500 GB for OpenShift Container Platform release images and OpenShift Container Platform 4.17 Red Hat Operator images. Up to 1 TB per stream or more is suggested. Shoot for 100GB+ for a fairly minimal install.</li> </ul> </li> <li> <p>Create a directory structure for the mirror registry contents. This example expects the <code>/opt</code> directory to have have sufficient space to hold the data for the images mirrored earlier as well as sufficient permissions for a non-root account to read/write to this area. Change this path/structure as you see fit for your environment.     <pre><code>umask 0022\nmkdir /opt/{mirror-registry,quay-storage,quay-root}\n</code></pre></p> </li> <li>Bring the mirror registry over to the machine that will be the registry host and untar it     <pre><code>cp /mnt/transfer-disk/mirror-registry-amd64.tar.gz /opt/mirror-registry/\ncd /opt/mirror-registry\ntar -xzvf mirror-registry-amd64.tar.gz\n</code></pre></li> <li>Firewall tcp port 8443 must be opened if using the default port of the mirror registry     <pre><code>sudo firewall-cmd --add-port=8443/tcp --permanent\nsudo firewall-cmd --reload\n</code></pre></li> <li>Verify the fully qualified hostname is set and can be resolved in dns     <pre><code>hostname -f\n</code></pre></li> <li>Install the mirror registry</li> </ol> <p>If this system is DISA STIG'd, or otherwise not a vanilla RHEL install the following changes may need to be made/adjusted</p> <ul> <li> <p><code>sysctl user.max_user_namespaces</code> must not be set to <code>0</code>. Namespaces are needed for rootless podman</p> <ul> <li>If you have issues with the install such as <code>error creating events dirs: mkdir /run/user/1000: permission denied</code>, take a look at this KCS Article. There may be timing issues with enabling user_namespaces and loginctl not creating the required directory during the boot process.</li> </ul> </li> <li> <p><code>noexec</code> must not be enabled on <code>/home</code>, or podman must be configured to use a different <code>rootless_storage_path</code> directory on a filesystem that allows exec. <code>rootless_storage_path</code> is defined in <code>/etc/containers/storage.conf</code>. </p> <ul> <li>This can be overridden on a per-user basis as well if needed by creating <code>~/.config/containers/storage.conf</code> and making edits there</li> <li>Refer to this article: Red Hat Blog: rootless-podman-nfs</li> </ul> </li> <li> <p><code>fapolicyd</code> may need to be adjusted or disabled</p> <ul> <li>You can add the binary to the policy like so: <pre><code>systemctl stop fapolicyd.service\n\nsudo fapolicyd-cli --file add /opt/mirror-registry/mirror-registry\nsudo fapolicyd-cli --update\n\nsystemctl start fapolicyd.service; systemctl status fapolicyd.service\n</code></pre></li> </ul> </li> <li> <p>If the users <code>$HOME</code> is on NFS network storage, podman must be configured to use a different <code>rootless_storage_path</code> directory on a filesystem. <code>rootless_storage_path</code> is defined in <code>/etc/containers/storage.conf</code>. </p> <ul> <li>This can be overridden on a per-user basis as well if needed by creating <code>~/.config/containers/storage.conf</code> and making edits there</li> <li>Refer to this article: Red Hat Blog: rootless-podman-nfs</li> </ul> </li> <li> <p>The user running podman must be a local Linux account, or have SUBUID/SUBGID explicitly mapped in <code>/etc/subuid</code> and <code>/etc/subgid</code>. Network accounts such as Active Directory don't exist in <code>/etc/passwd</code> so the podman tools have no idea how to create these maps for you.</p> <ul> <li>IdM/IPA can handle the creating of these if configured to do so: IdM Generate subID ranges</li> </ul> </li> <li> <p>Installing as root is not recommended but can be done, ssh access as root will need to be enabled for the install though which is highly frowned upon</p> </li> </ul> <ul> <li> <p>Installing the mirror registry. This will kick off an ansible playbook and install and run the pods     <pre><code>./mirror-registry install --quayHostname registry.example.com --quayRoot /opt/quay-root --quayStorage /opt/quay-storage\n</code></pre> Example Output<pre><code>...\nPLAY RECAP ********************************************************************************************************************************************************************admin@registry.example.com : ok=50   changed=28   unreachable=0    failed=0    skipped=14   rescued=0    ignored=0\n\nINFO[2025-03-17 14:39:08] Quay installed successfully, config data is stored in /opt/quay-root\nINFO[2025-03-17 14:39:08] Quay is available at https://registry.example.com:8443 with credentials (init, 4AywhWu5xsjiN2et09C3mg1rV7K6IS8f)\n</code></pre></p> </li> <li> <p>The registry containers will be setup to start on boot, this is handled by systemd user services in <code>~/.config/systemd/user/</code> <pre><code>tree ~/.config/systemd/user/\n/home/admin/.config/systemd/user/\n\u251c\u2500\u2500 default.target.wants\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 quay-app.service -&gt; /home/admin/.config/systemd/user/quay-app.service\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 quay-pod.service -&gt; /home/admin/.config/systemd/user/quay-pod.service\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 quay-redis.service -&gt; /home/admin/.config/systemd/user/quay-redis.service\n\u251c\u2500\u2500 multi-user.target.wants\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 quay-app.service -&gt; /home/admin/.config/systemd/user/quay-app.service\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 quay-pod.service -&gt; /home/admin/.config/systemd/user/quay-pod.service\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 quay-redis.service -&gt; /home/admin/.config/systemd/user/quay-redis.service\n\u251c\u2500\u2500 quay-app.service\n\u251c\u2500\u2500 quay-pod.service\n\u2514\u2500\u2500 quay-redis.service\n\n2 directories, 9 files\n</code></pre></p> </li> </ul>"},{"location":"optional/registry.html#create-your-pullpush-secret-for-your-mirror-registry","title":"Create your pull/push secret for your mirror registry","text":"<p>Generate a secret from the mirror registry and save it to your machine. </p> <ol> <li> <p>Navigate to Quay in your web browser, then create an account in Quay and login</p> <p>You can use the init account if you want, at the very least change default generated password </p> </li> <li> <p>Click on your username in the top right</p> <ul> <li>Account Settings &gt; Docker CLI Password &gt; Generate Encrypted Password &gt; Docker Configuration<ul> <li>Either Download username-auth.json, or view it and copy/paste it into the appropriate directory specified below</li> </ul> </li> </ul> </li> <li> <p>Specify the path to the folder to store the pull secret in and a name for the JSON file that you create. You can store this file in <code>/home/$USER/.docker/config.json</code> or <code>$XDG_RUNTIME_DIR/containers/auth.json</code>. If one of the directories aren't there, create them.</p> <ul> <li>The contents of the file resemble the following example: $XDG_RUNTIME_DIR/containers/auth.json<pre><code>{\n  \"auths\": {\n    \"registry.example.com:8443\": {\n      \"auth\": \"b3BlbnNo...\",\n      \"email\": \"you@example.com\"\n    }\n  }\n}\n</code></pre></li> </ul> </li> <li> <p>Verify that you can login to your registry. Your account should have push and pull permissions to your registry     <pre><code>podman login --tls-verify=false registry.example.com:8443\n</code></pre> Example Output<pre><code>Authenticating with existing credentials for registry.example.com:8443\nExisting credentials are valid. Already logged in to registry.example.com:8443\n</code></pre></p> </li> <li> <p>If you want your system to trust the registry, anchor the CA to the system and update the trust. This will allow registry authentication without having to use <code>--tls-verify=false</code> <pre><code>sudo cp /opt/quay-root/quay-rootCA/rootCA.pem /etc/pki/ca-trust/source/anchors/\nsudo update-ca-trust\n</code></pre></p> </li> <li> <p>By the end, you should have a Registry account that can push/pull (so oc-mirror can push images to it) and a account that can only pull (so the cluster can access the images for installing/updating). This registry should only be used to hold OpenShift release images. Follow your organizations best practice to administer this process.</p> </li> <li> <p>Continue to mirroring images to registry</p> </li> </ol>"},{"location":"postinstall/index.html","title":"Post Installation","text":"<p>After the cluster has been installed, it's time to make adjustments and additions to make sure the cluster is funcioning correctly in a disconnected environment. Some of these tasks cause node reboots , so it's best to perform these steps after the 24 hour mark passes to ensure that any nodes that may be rebooted will have correct kubelet certificates granted to them.</p> <p>This is commonly referrred to as <code>Day 2</code> operations. This only touches on a few tasks that may or may not need to be performed on your cluster. Reference the Postinstallation configuration overview documention for more thorough instructions.</p> <p>After installing OpenShift Container Platform, a cluster administrator can configure and customize the following components:</p> <ul> <li>Machine</li> <li>Bare metal</li> <li>Cluster</li> <li>Node</li> <li>Network</li> <li>Storage</li> <li>Users</li> <li>Alerts and notifications</li> </ul> <p>Best to enable tab completion on the <code>oc</code> command line tool. It makes navigating through the resources much easier. <pre><code>oc completion bash &gt; oc_bash_completion\n</code></pre> Then make it availble system-wide or source it from your <code>.bashrc</code> <pre><code>sudo cp oc_bash_completion /etc/bash_completion.d/\n</code></pre></p>"},{"location":"postinstall/ntp.html","title":"4.2 NTP Setup","text":""},{"location":"postinstall/ntp.html#configuring-a-time-source-for-the-disconnected-cluster","title":"Configuring a time source for the disconnected cluster","text":"<p>Red Hat Docs</p> <p>Info</p> <p>If you set <code>additionalNTPSources</code> in your agent-config.yaml, skip this step, you do not need to do this as NTP would've been configured during install.</p> <p>We need to apply a custom Network Time Protocol (NTP) configuration to the nodes, because by default, internet connectivity is assumed in OpenShift Container Platform and <code>chronyd</code> is configured to use the <code>*.rhel.pool.ntp.org</code> servers.</p> <ol> <li> <p>Create a Butane config including the contents of the <code>chrony.conf</code> file. For example, to configure chrony on master nodes, create a <code>99-master-chrony.bu</code> file. Do the same for worker nodes by swapping the metadata <code>name</code> and the <code>machineconfiguration.openshift.io/role</code> label.</p> <p>The Butane version you specify in the config file should match the OpenShift Container Platform version and always ends in <code>0</code>. For example, <code>4.17.0</code>.</p> </li> </ol> <pre><code>variant: openshift\nversion: 4.17.0\nmetadata:\n  name: 99-master-chrony # (1)! On control plane nodes, substitute master for worker in both of these locations\n  labels:\n    machineconfiguration.openshift.io/role: master # (2)! On control plane nodes, substitute master for worker in both of these locations\nstorage:\n  files:\n  - path: /etc/chrony.conf\n    mode: 0644 # (3)! Specify an octal value mode for the mode field in the machine config file\n    overwrite: true\n    contents:\n      inline: | # (4)! Specify any valid, reachable time source. `172.16.10.123` is an example time server. IP address or hostname is accepatable \n        pool 172.16.10.123 iburst\n        driftfile /var/lib/chrony/drift\n        makestep 1.0 3\n        rtcsync\n        logdir /var/log/chrony\n</code></pre> <ol> 2. Use Butane to generate a <code>MachineConfig</code> object file, <code>99-master-chrony.yaml</code>, containing the configuration to be delivered to the nodes  <pre><code>butane 99-master-chrony.bu -o 99-master-chrony.yaml\n</code></pre> </ol> <ol> 3. Apply the configurations in one of two ways:  <ul> <li> If the cluster is not running yet, after you generate manifest files, add the <code>MachineConfig</code> object file to the <code>installation_directory/openshift</code> directory, and then continue to create the cluster. </li> </ul> <ul> <li> If the cluster is already running, apply the file </li> </ul> <pre><code>oc apply -f ./99-master-chrony.yaml\n</code></pre> </ol> <ol> <li>On worker nodes, change <code>master</code> to <code>worker</code></li> <li>On worker nodes, change <code>master</code> to <code>worker</code></li> <li>Specify an octal value mode for the <code>mode</code> field in the machine config file. After creating the file and applying the changes, the <code>mode</code> is converted to a decimal value. You can check the YAML file with the command <code>oc get mc &lt;mc-name&gt; -o yaml</code></li> <li>Specify any valid, reachable time source. <code>172.16.10.123</code> is an example time server. IP address or hostname is accepatable </li> </ol>"},{"location":"postinstall/osus.html","title":"4.3 OpenShift Update Service","text":""},{"location":"postinstall/osus.html#configuring-the-openshift-update-service","title":"Configuring the OpenShift Update Service","text":"<p>Red Hat Docs</p> <p>Red Hat Blog on updating clusters</p> <p>Handy Update tool</p> <p>This is kind of a pain but should only need to be set up one time. We essentially have to tell the cluster to look at our registry for graph data and release images like it does when connected to the Internet. Perform these steps only if you mirrored graph data and the <code>cincinnati-operator</code> to your mirror registry.</p> <p>The following steps outline the high-level workflow on how to update a cluster in a disconnected environment using OSUS:</p> <ol> <li>Configure access to a secured registry.</li> <li>Update the global cluster pull secret to access your mirror registry (if needed).</li> <li>Install the OSUS Operator.</li> <li>Create a service container for the OpenShift Update Service.</li> <li>Install the OSUS application and configure your clusters to use the OpenShift Update Service in your environment.</li> <li>Perform a supported update procedure from the documentation as you would with a connected cluster.</li> </ol>"},{"location":"postinstall/osus.html#configure-access-to-the-secured-registry","title":"Configure access to the secured registry","text":"<p>Red Hat Docs</p> <p>Create a configmap in the openshift-config namespace and use its name in AdditionalTrustedCA in the <code>image.config.openshift.io</code> custom resource to provide additional CAs that should be trusted when contacting external registries for images. </p> <ol> <li> <p>Grab the CA in pem format for the container registry that the graph-image is in and create a new <code>configmap</code> object in the <code>openshift-config</code> namespace that defines your registry to be used for the updateservice.</p> <ul> <li>The OpenShift Update Service Operator needs the config map key name <code>updateservice-registry</code> in the registry CA cert.</li> </ul> <pre><code># Since the OSUS image is on the Quay Mirror Registry we need it's rootCA\ncp /opt/quay-data/quay-rootCA/rootCA.pem ca.crt\n\noc create configmap image-ca-bundle --from-file=updateservice-registry=ca.crt -n openshift-config\n</code></pre> <p>Info</p> <p>Here we copied the rootCA of our registry to our current directory and called it <code>ca.crt</code>. Then created a configmap called <code>image-ca-bundle</code> in the openshift-config namespace using that certificate.</p> </li> <li> <p>Edit the <code>Image</code> custom resource and add the configmap you just created to the additionalTrustedCA spec.       <pre><code>oc edit image.config.openshift.io cluster\n</code></pre> <pre><code>spec:\n  additionalTrustedCA:\n    name: image-ca-bundle\n</code></pre></p> <ul> <li> <p>Red Hat Docs </p> <p>You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret. The procedure is required when users use a separate registry to store images than the registry used during installation. If you are using the same registry that you installed from (recommended), you can skip this.</p> </li> </ul> </li> </ol>"},{"location":"postinstall/osus.html#install-the-operator-from-the-ui-or-cli","title":"Install the operator from the UI or CLI","text":""},{"location":"postinstall/osus.html#ui-install","title":"UI Install","text":"<p>Red Hat Docs</p> <ol> <li>In the web console, click Operators &gt; OperatorHub.</li> <li>Choose OpenShift Update Service from the list of available Operators, and click Install.<ol> <li>Select an Update channel.</li> <li>Select a Version.</li> <li>Select A specific namespace on the cluster under Installation Mode.</li> <li>Select a namespace for Installed Namespace or accept the recommended namespace <code>openshift-update-service</code>.</li> <li>Select an Update approval strategy:<ul> <li>The Automatic strategy allows Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.</li> <li>The Manual strategy requires a cluster administrator to approve the Operator update.</li> </ul> </li> <li>Click Install.</li> </ol> </li> <li>Go to Operators &gt; Installed Operators and verify that the OpenShift Update Service Operator is installed.</li> <li>Ensure that OpenShift Update Service is listed in the correct namespace with a Status of Succeeded.</li> </ol>"},{"location":"postinstall/osus.html#cli-install","title":"CLI install","text":"<p>Red Hat Docs</p> <ol> <li> <p>Create a namespace for the OpenShift Update Service Operator:</p> <ul> <li> <p>Create a <code>Namespace</code> object YAML file, for example, <code>update-service-namespace.yaml</code>, for the OpenShift Update Service Operator:   <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-update-service\n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-monitoring: \"true\"\n</code></pre></p> <p>Set the openshift.io/cluster-monitoring label to enable Operator-recommended cluster monitoring on this namespace.</p> </li> <li> <p>Create the namespace:   <pre><code># Example\noc create -f update-service-namespace.yaml\n</code></pre></p> </li> </ul> </li> <li> <p>Install the OpenShift Update Service Operator by creating the following objects:</p> <ul> <li> <p>Create an <code>OperatorGroup</code> object YAML file, for example, <code>update-service-operator-group.yaml</code>:   <pre><code>apiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: update-service-operator-group\n  namespace: openshift-update-service\nspec:\n  targetNamespaces:\n  - openshift-update-service\n</code></pre></p> </li> <li> <p>Create an <code>OperatorGroup</code> object:   <pre><code># Example\noc -n openshift-update-service create -f update-service-operator-group.yaml\n</code></pre></p> </li> <li>Create a <code>Subscription</code> object YAML file, for example, <code>update-service-subscription.yaml</code>:<ul> <li>Example Subscription <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: update-service-subscription\n  namespace: openshift-update-service\nspec:\n  channel: v1\n  installPlanApproval: \"Automatic\"\n  source: \"redhat-operators\" # Specify the correct source\n  sourceNamespace: \"openshift-marketplace\"\n  name: \"cincinnati-operator\"\n</code></pre>   Specify the name of the catalog source that provides the Operator. For clusters that do not use a custom Operator Lifecycle Manager (OLM), specify <code>redhat-operators</code>. If your OpenShift Container Platform cluster is installed in a disconnected environment, specify the name of the <code>CatalogSource</code> object created when you configured Operator Lifecycle Manager (OLM).   <pre><code># Heres a way to see what catalog source the cluster is configured for\n$ oc get service -n openshift-marketplace\n</code></pre> Example Output<pre><code>NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\nredhat-operators                 ClusterIP   172.30.79.90   &lt;none&gt;        50051/TCP           12d\nmarketplace-operator-metrics     ClusterIP   172.30.26.63   &lt;none&gt;        8383/TCP,8081/TCP   13d\n</code></pre></li> </ul> </li> <li>Create the Subscription object:   <pre><code># Example\noc -n openshift-update-service create -f update-service-subscription.yaml\n</code></pre>   The OpenShift Update Service Operator is installed to the openshift-update-service namespace and targets the openshift-update-service namespace.</li> </ul> </li> <li> <p>Verify the Operator installation:       <pre><code>oc -n openshift-update-service get clusterserviceversions\n</code></pre> Example Output<pre><code>NAME                             DISPLAY                    VERSION   REPLACES                         PHASE\nupdate-service-operator.v5.0.3   OpenShift Update Service   5.0.3     update-service-operator.v5.0.2   Succeeded\n</code></pre></p> </li> </ol>"},{"location":"postinstall/osus.html#create-an-openshift-update-service-application","title":"Create an OpenShift Update Service application","text":"<p>Info</p> <p>oc-mirror will generate a <code>updateService.yaml</code> file for you when mirroring from disk to mirror. It will be located in the <code>working-dir/cluster-resources/</code> directory.</p> <p>The name of the service it will create is: <code>update-service-oc-mirror</code></p> <p>Make sure to apply the service in the same namespace as the operator i.e. <code>openshift-update-service</code></p> <pre><code>oc -n openshift-update-service apply -f /opt/4.17-mirrordata/working-dir/cluster-resources/updateService.yaml\n</code></pre> <p>You can use that service file, or create one by following the steps below.</p> <p>Here we'll deploy the service pods, pointing to graph-data to the one we mirrored to our registry, and pointing the release-images our registry mirror rather than Red Hat's CDN. </p> <p>Red Hat Docs</p> <ol> <li>Configure the OpenShift Update Service target namespace, for example, <code>openshift-update-service</code>:       <pre><code>NAMESPACE=openshift-update-service\n</code></pre>       The namespace must match the <code>targetNamespaces</code> value from the operator group.</li> <li>Configure the name of the OpenShift Update Service application, for example, <code>update-service</code>:       <pre><code>NAME=update-service\n</code></pre></li> <li>Configure the registry and repository for the release images as configured for example, <code>registry.example.com:8443/ocp/openshift/release-images</code>:       <pre><code>RELEASE_IMAGES=registry.example.com:8443/ocp/openshift/release-images\n</code></pre></li> <li>Set the local pullspec for the graph data image to the graph data container image for example, <code>registry.example.com:8443/ocp/openshift/graph-image:latest</code>:       <pre><code>GRAPH_DATA_IMAGE=registry.example.com:8443/ocp/openshift/graph-image:latest\n</code></pre></li> <li>Create an OpenShift Update Service application object:       <pre><code>oc -n \"${NAMESPACE}\" create -f - &lt;&lt;EOF\napiVersion: updateservice.operator.openshift.io/v1\nkind: UpdateService\nmetadata:\n  name: ${NAME}\nspec:\n  replicas: 1\n  releases: ${RELEASE_IMAGES}\n  graphDataImage: ${GRAPH_DATA_IMAGE}\nEOF\n</code></pre></li> </ol>"},{"location":"postinstall/osus.html#configuring-the-cluster-version-operator-cvo","title":"Configuring the Cluster Version Operator (CVO)","text":"<p>Red Hat Docs</p> <p>After the OpenShift Update Service Operator has been installed and the OpenShift Update Service application has been created, the Cluster Version Operator (CVO) can be updated to pull graph data from the OpenShift Update Service installed in your environment.</p> <ol> <li>Set the OpenShift Update Service target namespace, for example, <code>openshift-update-service</code>:       <pre><code>NAMESPACE=openshift-update-service\n</code></pre></li> <li>Use the name of the OpenShift Update Service application created previously, for example, <code>update-service</code>:       <pre><code>NAME=update-service\n\n# If you used the oc mirror generated updateService.yaml\nNAME=update-service-oc-mirror\n</code></pre></li> <li>Obtain the policy engine route:       <pre><code>POLICY_ENGINE_GRAPH_URI=\"$(oc -n \"${NAMESPACE}\" get -o jsonpath='{.status.policyEngineURI}/api/upgrades_info/v1/graph{\"\\n\"}' updateservice \"${NAME}\")\"\n</code></pre></li> <li>Set the patch for the pull graph data:       <pre><code>PATCH=\"{\\\"spec\\\":{\\\"upstream\\\":\\\"${POLICY_ENGINE_GRAPH_URI}\\\"}}\"\n</code></pre></li> <li>Patch the CVO to use the OpenShift Update Service in your environment:       <pre><code>oc patch clusterversion version -p $PATCH --type merge\n</code></pre></li> </ol>"},{"location":"postinstall/osus.html#configure-the-cluster-wide-proxy","title":"Configure the cluster-wide proxy","text":"<p>Red Hat Docs</p> <p>Finally, configure the cluster-wide proxy to configure the CA to trust the update server we created.</p> <p>This may cause nodes to reboot so be prepared</p> <ol> <li>Get the ingress-router CA and save it to a file       <pre><code>oc get -n openshift-ingress-operator secret router-ca -o jsonpath=\"{.data.tls\\.crt}\" | base64 -d &gt; ca-bundle.crt\n</code></pre></li> <li> <p>Create a configmap from that CA and store it in the openshift-config namespace       <pre><code>oc create configmap router-bundle --from-file=ca-bundle.crt -n openshift-config\n</code></pre></p> <p>Info</p> <p>Here we created a configmap called <code>router-bundle</code> in the openshift-config namespace using the CA cert from the ingress-router that we saved to the same directory.</p> </li> <li> <p>Edit the cluster proxy and add the config map you just created to the TrustedCA spec       <pre><code>oc edit proxy cluster\n</code></pre> <pre><code>spec:\n  trustedCA:\n    name: router-bundle\n</code></pre></p> </li> </ol>"},{"location":"postinstall/resources.html","title":"4.1 Apply Cluster Resources","text":""},{"location":"postinstall/resources.html#patch-the-operatorhub","title":"Patch the OperatorHub","text":"<p>Before importing these cluster sources, it's best to disable all default catalog sources with this command (as OperatorHub will be actively trying to connect to the Internet to fetch them) <pre><code>oc patch OperatorHub cluster --type json -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]'\n</code></pre></p>"},{"location":"postinstall/resources.html#cluster-resources","title":"Cluster Resources","text":"<p>Red Hat Docs</p> <p>From your <code>oc mirror</code> command, you could have the following files in your working-dir i.e. <code>/opt/4.17-mirrordata/working-dir/cluster-resources/</code>. It all depends on what you mirrored. </p> <ul> <li><code>idms-oc-mirror.yaml</code>: This is a list of mappings between the original public registry and your local registry for all images that are identified by their digest (ImageDigestMirrorSet).</li> <li><code>itms-oc-mirror.yaml</code>: This is a list of mappings between the original public registry and your local registry for all images that are identified by their tag (ImageTagMirrorSet).</li> <li> <p><code>cs-redhat-operator-index-v4-17.yaml/cc-redhat-operator-index-v4-17.yaml</code>: This is the CatalogSource/ClusterCatalog for RedHat Operators. </p> <p>Important!</p> <p>Change the name the <code>cs-redhat-operator-index-v4-17.yaml/cc-redhat-operator-index-v4-17.yaml</code> catalog source in the YAML file to <code>redhat-operators</code> as some operators are hardcoded reference this exact catalog source name. Examples below with the highlighted value:</p> <p>Tip</p> <p>Use a quick <code>sed</code> command to set the name in both files <pre><code>sed -i 's/name: cs-redhat-operator-index-v4-17/name: redhat-operators/' /opt/4.17-mirrordata/working-dir/cluster-resources/cs-redhat-operator-index-v4-17.yaml\nsed -i 's/name: cc-redhat-operator-index-v4-17/name: redhat-operators/' /opt/4.17-mirrordata/working-dir/cluster-resources/cc-redhat-operator-index-v4-17.yaml\n</code></pre></p> Example: cs-redhat-operator-index-v4-17.yaml<pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: CatalogSource\nmetadata:\n  name: redhat-operators\n  namespace: openshift-marketplace\nspec:\n  image: registry.example.com:8443/ocp/redhat/redhat-operator-index:v4.17\n  sourceType: grpc\nstatus: {}\n</code></pre> Example: cc-redhat-operator-index-v4-19.yaml<pre><code>apiVersion: olm.operatorframework.io/v1\nkind: ClusterCatalog\nmetadata:\n  annotations:\n    createdAt: Monday, 21-Jul-25 21:13:27 UTC\n    createdBy: oc-mirror v2\n    oc-mirror_version: 4.19.0-202507081507.p0.gf364aec.assembly.stream.el9-f364aec\n  name: redhat-operators\nspec:\n  priority: 0\n  source:\n    image:\n      ref: registry.example.com:8443/ocp/redhat/redhat-operator-index:v4.19\n    type: Image\nstatus: {}\n</code></pre> </li> <li> <p><code>cs-certified-operator-index-v4-17.yaml</code>: This is the catalog for operators certified by Red Hat but not necessarily developed by Red Hat.</p> </li> <li> <p><code>cs-community-operator-index-v4-17.yaml</code>: This is the catalog source for all community based operators.</p> <p>Info</p> <p>If you have any ClusterCatalogs i.e <code>cc-redhat-operator-index-v4-17.yaml</code>, they will only work on OpenShift 4.18 or newer. Ignore errors from ClusterCatalogs when applying resources if you are on an older version of OpenShift</p> <p>This is due to changes in the Operator Lifecycle Manager (OLM) architecture moving from OLM classic to OLMv1: Red Hat Docs</p> </li> </ul>"},{"location":"postinstall/resources.html#configure-the-cluster-to-use-the-resources-you-mirrored","title":"Configure the cluster to use the resources you mirrored","text":"<p>Red Hat Docs</p> <ol> <li> <p>Apply the cluster recources generated by the oc-mirror plugin. This will make your cluster aware of the catalog resources we mirrored earlier so you can utilze OperatorHub to install operators. <pre><code>oc apply -f /opt/4.17-mirrordata/working-dir/cluster-resources/\n</code></pre> Example Output<pre><code>catalogsource.operators.coreos.com/cs-certified-operator-index-v4-17 created\ncatalogsource.operators.coreos.com/cs-community-operator-index-v4-17 created\ncatalogsource.operators.coreos.com/redhat-operators created\nimagedigestmirrorset.config.openshift.io/idms-release-0 created\nimagedigestmirrorset.config.openshift.io/idms-operator-0 created\nimagetagmirrorset.config.openshift.io/itms-release-0 created\nimagetagmirrorset.config.openshift.io/itms-generic-0 created\nconfigmap/mirrored-release-signatures created\n</code></pre></p> </li> <li> <p>If you mirrored release images, apply the release image signatures <pre><code>oc apply -f /opt/4.17-mirrordata/cluster-resources/signature-configmap.json\n</code></pre></p> </li> </ol> <p>Important</p> <p>If you are mirroring Operators instead of clusters, do not run the preceding command. Running the command results in an error because there are no release image signatures to apply.</p> <p>Additionally, a YAML file is available in the same directory <code>working-dir/cluster-resources/</code>. You can use either the JSON or YAML format.</p>"},{"location":"postinstall/resources.html#verify-resources-have-been-applied","title":"Verify resources have been applied","text":"<ol> <li> <p>Verify the CatalogSource has been installed. You may have some, all, or none of these depending on what Operators you mirrored. <pre><code>oc get catalogsource -A\n</code></pre> Example Output<pre><code>NAMESPACE               NAME                                DISPLAY   TYPE   PUBLISHER   AGE\nopenshift-marketplace   cs-certified-operator-index-v4-17             grpc               2d\nopenshift-marketplace   cs-community-operator-index-v4-17             grpc               2d\nopenshift-marketplace   redhat-operators                              grpc               2d\n</code></pre></p> </li> <li> <p>Verify that the ImageDigestMirrorSet resources are successfully installed <pre><code>oc get imagedigestmirrorset\n</code></pre> Example Output<pre><code>NAME                  AGE\nidms-operator-0       2d\nidms-release-0        2d\nimage-digest-mirror   2d\n</code></pre></p> </li> <li> <p>Verify that the ImageTagMirrorSet resources are successfully installed <pre><code>oc get imagetagmirrorset\n</code></pre> Example Output<pre><code>NAME             AGE\nitms-generic-0   2d\nitms-release-0   2d\n</code></pre></p> </li> </ol>"},{"location":"postinstall/tasks.html","title":"4.4 Additional Tasks","text":""},{"location":"postinstall/tasks.html#additional-post-install-tasks","title":"Additional Post-Install Tasks","text":"<p>Many of these settings have already been applied if you deployed the cluster with the appropriate customizations but it's good to double check everything. Much of this is regurgitated information from Source: Kens disconnected-openshift Repo which is a great resource for additional diconnected install information/configs/tools.</p>"},{"location":"postinstall/tasks.html#cluster-wide-ca-certs","title":"Cluster-wide CA Certs","text":"<p>Red Hat Docs</p> <p>Typically you'll have custom internal Root Certificate Authorities that sign TLS certs for services. If you provided the certificates during installation (your mirror registry for example), you should find them in the <code>user-ca-bundle</code> ConfigMap in the <code>openshift-config</code> Namespace under the <code>ca-bundle.crt</code> key. </p> <ol> <li> <p>Verify that the contents match what you have for your Root CAs <pre><code>oc get cm/user-ca-bundle -n openshift-config -o yaml\n</code></pre> Example Output<pre><code>apiVersion: v1\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIID1jCCAr6gAwIBAgIUZ11....\n    -----END CERTIFICATE-----\n    -----BEGIN CERTIFICATE-----\n    MIID1jCCAr6gAwIBAgIUZ11....\n    -----END CERTIFICATE-----\nkind: ConfigMap\n...\n</code></pre></p> </li> <li> <p>If you need to add additional CA's, modify this ConfigMap with the cert data user-ca-bundle.yaml<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-ca-bundle\n  namespace: openshift-config\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIID1jCCAr6gAwIBAgIUZ11....\n    -----END CERTIFICATE-----\n    -----BEGIN CERTIFICATE-----\n    MIID1jCCAr6gAwIBAgIUZ11....\n    -----END CERTIFICATE-----\n    -----BEGIN CERTIFICATE-----\n    ... other other cert text ...\n    -----END CERTIFICATE-----\n</code></pre></p> <p>This may cause nodes to reboot so be prepared</p> <pre><code># Edit from the cmdline and copy/paste contents\noc edit configmaps user-ca-bundle -n openshift-config\n\n# Or\n\n# Apply the yaml file if you created one\noc apply -f user-ca-bundle.yaml\n</code></pre> </li> <li> <p>With the Root CA Certificates in the cluster-wide ConfigMap, you can now create other ConfigMaps with a special label that will inject all the trusted Root CA certificates into it which makes it easy to be used by applications image-additional-trust-bundle.yaml<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: image-additional-trust-bundle\n  namespace: openshift-config\n  labels:\n    # This label will create the .data['ca-bundle.crt'] key with all the system trusted roots, custom and default\n    config.openshift.io/inject-trusted-cabundle: 'true'\n</code></pre> <pre><code># Apply the yaml file\noc apply -f image-additional-trust-bundle.yaml\n</code></pre></p> </li> </ol>"},{"location":"postinstall/tasks.html#disabling-the-insights-operator","title":"Disabling the Insights Operator","text":"<p>This probably shows up in the WebUI as disabled, but is most likely set to <code>Managed</code> (enabled). Change the <code>managementState: Managed</code> to <code>managementState: Unmanaged</code>.</p> <p><pre><code>oc get insightsoperator.operator.openshift.io/cluster -o yaml\n</code></pre> <pre><code>apiVersion: operator.openshift.io/v1\nkind: InsightsOperator\nmetadata:\n  ...\nspec:\n  logLevel: Normal\n  managementState: Managed\n  operatorLogLevel: Normal\n</code></pre></p> <p>Set it to Unmanaged insights-disable.yaml<pre><code>---\napiVersion: operator.openshift.io/v1\nkind: InsightsOperator\nmetadata:\n  name: cluster\nspec:\n  # Change from Managed to Unmanaged\n  managementState: Unmanaged\n</code></pre> <pre><code># Edit from the cmdline and copy/paste contents\noc edit insightsoperator.operator.openshift.io/cluster\n\n# Or\n\n# Apply the yaml file if you created one\noc apply -f insights-disable.yaml\n</code></pre></p>"},{"location":"postinstall/tasks.html#image-cr-and-additional-container-registries","title":"Image CR and additional Container Registries","text":"<p>The cluster has different mechanisms to control how images are pulled - you can block registries, specifically only registries, set insecure registries, etc. This is all configured in the Image CR.</p> <p>You also need to set some configuration here for registries you're pulling from that are signed by custom Root CA certificates. These definitions are used for ImageStreams, the OpenShift Update Service, and a couple of other places. Not required for core cluster image pulling functionality but good to configure nonetheless.</p> <p>To define Root CA certificates, you have to create a ConfigMap. This ConfigMap needs to have keys that are named for the hostname of the registry. If you're using a non-443 port for the registry, append it to the hostname with two dots to separate it, eg <code>registry.example.com:5000</code> would be <code>registry.example.com..5000</code>.</p> <p>When configuring the Root CA for the registry that hosts the OpenShift Releases served by the OpenShift Update Service there is an <code>updateservice-registry</code> key that is used. This is outlined in the OSUS Install Doc: Configure access to the secured registry</p> <p>image-ca-bundle.yaml<pre><code># Root CA definitions for use by the config/Image CR\n# Each image registry URL should have a corresponding entry in this ConfigMap\n# with the registry URL as the key and the CA certificate as the value.\n# If there is a port for the registry, use two dots to separate the registry hostname and the port.\n# For example, if the registry URL is registry.example.com:5000, the key should be registry.example.com..5000\n# The updateservice-registry entry is used for the OpenShift Update Service\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: image-ca-bundle\n  namespace: openshift-config\ndata:\n  # updateservice-registry is for the registry hosting OSUS Releases\n  updateservice-registry: |\n    -----BEGIN CERTIFICATE-----\n    MIIH0DCCBbigAwIBAgIUV...\n    -----END CERTIFICATE-----\n  my-other-registry.example.com: |\n    -----BEGIN CERTIFICATE-----\n    ... cert text here ...\n    -----END CERTIFICATE-----\n  test-registry.example.com..8443: |\n    -----BEGIN CERTIFICATE-----\n    ... cert text here ...\n    -----END CERTIFICATE-----\n</code></pre> <pre><code># Edit from the cmdline if the resource already exists and copy/paste contents\noc edit configmaps image-ca-bundle -n openshift-config\n\n# Or\n\n# Apply the yaml file if you created one\noc apply -f image-ca-bundle.yaml\n</code></pre></p> <p>With that ConfigMap created, you can now configure the Image CR with it and any other configuration you want</p> image-config-cr.yaml<pre><code>---\napiVersion: config.openshift.io/v1\nkind: Image\nmetadata:\n  name: cluster\nspec:\n  # additionalTrustedCA is a reference to a ConfigMap containing additional CAs that should be trusted during imagestream import, pod image pull, build image pull, and imageregistry pullthrough.\n  # The namespace for this config map is openshift-config.\n  additionalTrustedCA:\n    name: image-ca-bundle\n\n  # Optional configuration...\n\n  # allowedRegistriesForImport limits the container image registries that normal users may import images from. Set this list to the registries that you trust to contain valid Docker images and that you want applications to be able to import from.\n  # Users with permission to create Images or ImageStreamMappings via the API are not affected by this policy - typically only administrators or system integrations will have those permissions.\n  allowedRegistriesForImport:\n    - image-registry.openshift-image-registry.svc:5000\n    - my-other-registry.example.com\n    - test-registry.example.com:8443\n\n  registrySources:\n    # allowedRegistries are the only registries permitted for image pull and push actions. All other registries are denied. \n    # Only one of blockedRegistries or allowedRegistries may be set.\n    allowedRegistries:\n      - image-registry.openshift-image-registry.svc:5000\n      - my-other-registry.example.com\n      - test-registry.example.com:8443\n\n    # blockedRegistries cannot be used for image pull and push actions. All other registries are permitted. \n    # Only one of BlockedRegistries or AllowedRegistries may be set.\n    blockedRegistries:\n      - docker.io\n\n    # containerRuntimeSearchRegistries are registries that will be searched when pulling images that do not have fully qualified domains in their pull specs.\n    # Registries will be searched in the order provided in the list.\n    # Note: this search list only works with the container runtime, i.e CRI-O. Will NOT work with builds or imagestream imports.\n    containerRuntimeSearchRegistries:\n      - image-registry.openshift-image-registry.svc:5000\n      - test-registry.example.com:8443\n\n    # insecureRegistries are registries which do not have a valid TLS certificates or only support HTTP connections.\n    insecureRegistries:\n      - test-registry.example.com:8443\n</code></pre> <pre><code># Edit from the cmdline and copy/paste contents\noc edit image.config.openshift.io cluster\n\n# Or\n\n# Apply the yaml file if you created one\noc apply -f image-config-cr.yaml\n</code></pre>"}]}